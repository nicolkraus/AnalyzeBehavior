{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " \n",
       "# BehaviorAnalyzer \n",
       "<em>An interactive tool to visually analyse behavior data derived from event-logging software like BORIS</em> \n",
       "\n",
       "- <strong>Usage</strong>: Upload file containing the data \n",
       " by clicking <em>Behavior</em>\n",
       "\n",
       "- <strong>Required columns</strong>: <em>Time</em>, <em>Subject</em>, <em>Behavior</em>, <em>Status</em>\n",
       "\n",
       "- <strong>Optional columns</strong>: <em>Modifier 1</em>, <em>Behavioral category</em>, <em>Total length</em> ... \n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This notebook is the programmatic part of the bachelors project 'A concept and prototypical implementation for network based \n",
    "analysis of fish behavior' by Nicolai Kraus at the University of Constance, supported by Michael Aichem and \n",
    "supervised by Dr. Karsten Klein. \n",
    "\n",
    "The project consists of a pipeline which loads a behavior dataset and produces a interactive dashboard via jupyter \n",
    "notebook and voila.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#standard libraries\n",
    "import os\n",
    "import io\n",
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import base64\n",
    "from termcolor import colored\n",
    "\n",
    "#network/plot generation\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import to_pydot\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "\n",
    "#network distances/similarity\n",
    "import netrd\n",
    "import itertools\n",
    "\n",
    "#UI/display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive,interact, fixed, VBox, HBox\n",
    "from IPython.display import display, Image, Markdown, SVG, HTML, clear_output, FileLink\n",
    "\n",
    "#remove warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#this line is needed for windows so the library 'pygraphviz', a wrapper of 'graphviz' for 'python'\n",
    "#can load its modules 'dot' and 'neato' properly.\n",
    "if  not 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin' in os.environ[\"PATH\"]: \n",
    "    os.environ[\"PATH\"] += os.pathsep + 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin'  \n",
    "    \n",
    "#global variable dataframe\n",
    "df = None\n",
    "trajectory_df = None\n",
    "G1 = None\n",
    "G2 = None\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\" \n",
    "# BehaviorAnalyzer \n",
    "<em>An interactive tool to visually analyse behavior data derived from event-logging software like BORIS</em> \\n\n",
    "- <strong>Usage</strong>: Upload file containing the data \\n by clicking <em>Behavior</em>\\n\n",
    "- <strong>Required columns</strong>: <em>Time</em>, <em>Subject</em>, <em>Behavior</em>, <em>Status</em>\\n\n",
    "- <strong>Optional columns</strong>: <em>Modifier 1</em>, <em>Behavioral category</em>, <em>Total length</em> ... \\n\n",
    "---\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fish_ids(boris_df):\n",
    "    \"\"\"This function collects the unique fish ids. Functions needing these call this function so the order\n",
    "    is always the same which results in a consequent colour scheme over all plots.\"\"\"\n",
    "    fish_ids = boris_df.subject.unique().tolist()\n",
    "    if 'Subject' in fish_ids: fish_ids.remove('Subject')\n",
    "    fish_ids = [x for x in fish_ids if str(x) != 'nan']\n",
    "    return fish_ids\n",
    "\n",
    "\n",
    "def get_total_and_avg_time(df, fish_ids):\n",
    "    df = df[['time', 'subject', 'chosen_data', 'status']]\n",
    "    behavior_ids = df.chosen_data.unique().tolist()\n",
    "    time_list = []\n",
    "    for behavior in behavior_ids:\n",
    "        behavior_df = df[df.chosen_data == behavior]\n",
    "        total = 0\n",
    "        avg = 0\n",
    "        for fish in fish_ids:\n",
    "            id_frame = behavior_df[behavior_df.subject == fish]\n",
    "            stop_total = id_frame[id_frame.status == 'STOP'].time.sum()\n",
    "            start_total = id_frame[id_frame.status == 'START'].time.sum()\n",
    "            total = total + stop_total - start_total\n",
    "        occurences = len(behavior_df[behavior_df.status == 'START'].index)\n",
    "        if (math.isnan(occurences) or (occurences < 1)):\n",
    "            occurences = 1\n",
    "        if (total == 0.0):\n",
    "            avg = 0.0\n",
    "        else:\n",
    "            avg = total / occurences\n",
    "        time_list.append((behavior, total, avg))\n",
    "    return time_list\n",
    "        \n",
    "            \n",
    "def get_row_index(df, values):\n",
    "    \"\"\" \n",
    "    Get index positions of values in dataframe\n",
    "    \n",
    "    `Required` \n",
    "    :param df: Panda dataframe\n",
    "    :param values: data structure with values to search\n",
    "    \"\"\"\n",
    "    \n",
    "    for value in values:\n",
    "        listOfPos = list()\n",
    "        # Get bool dataframe with True at positions where the given value exists\n",
    "        result = df.isin([value])\n",
    "        # Get list of columns that contains the value\n",
    "        seriesObj = result.any()\n",
    "        columnNames = list(seriesObj[seriesObj == True].index)\n",
    "        # Iterate over list of columns and fetch the rows indexes where value exists\n",
    "        for col in columnNames:\n",
    "            rows = list(result[col][result[col] == True].index)\n",
    "            for row in rows:\n",
    "                listOfPos.append(row)\n",
    "                return listOfPos\n",
    "        # Return a list of tuples indicating the positions of value in the dataframe\n",
    "    return listOfPos\n",
    "     \n",
    "def _clean(df):\n",
    "    \"\"\"\n",
    "    Delete unneeded header information and standardize column names. \n",
    "    Add necessary column names if not present.\n",
    "    \n",
    "    `Required` \n",
    "    :param df: Panda dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    #If header is not first row, delete rows until one of ['Time', 'time', 'Subject', 'Fps', 'fps', 'subject'] appears\n",
    "    try:\n",
    "        header_row_index = get_row_index(df, ['Time', 'time', 'Subject', 'subject', 'Status', 'Behavior'])[0]\n",
    "        df = df.iloc[header_row_index:]\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #all header in lowercase, no spaces\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    df.columns = df.columns.str.replace(' ','_')\n",
    "    \n",
    "    #convert time to float if excel gives string objects   \n",
    "    df.time = df.time.astype(float)\n",
    "   \n",
    "    #if dataset contains only two individuals and modifier_1 not included, add corresponding modifier_1\n",
    "    if 'modifier_1' not in df.columns and len(df.subject.unique()) == 2:\n",
    "        df['modifier_1'] = df.subject.unique()[0]\n",
    "        df['modifier_1'] = np.where(df['subject'] == df.subject.unique()[0], df.subject.unique()[1], df['modifier_1'])\n",
    "\n",
    "    #add missing columns\n",
    "    if 'modifier_1' not in df.columns:\n",
    "        df['modifier_1'] = 'unknown'\n",
    "    if 'behavioral_category' not in df.columns:\n",
    "        df['behavioral category '] = 'unknown'\n",
    "    if 'status' not in df.columns:\n",
    "        df['status'] = 'unknown'\n",
    "    if 'total_length' not in df.columns:\n",
    "        df['total_length'] = df['time'].iloc[-1]\n",
    "    \n",
    "    #map behaviors to corresponding behavioral category\n",
    "    df['behavior'] = [x.lower() for x in df['behavior']]\n",
    "    _map = [\n",
    "        #overt aggressive\n",
    "        (df['behavior'] == 'bite', 'overt aggressive'),\n",
    "        (df['behavior'] == 'mouth', 'overt aggressive'),\n",
    "        (df['behavior'] == 'ram', 'overt aggressive'),\n",
    "        (df['behavior'] == 'mouthfight', 'overt aggressive'),\n",
    "        #aggressive\n",
    "        (df['behavior'] == 'bite/ram', 'overt aggressive'),\n",
    "        (df['behavior'] == 'chase', 'aggressive'),\n",
    "        (df['behavior'] == 'frontal', 'aggressive'),\n",
    "        (df['behavior'] == 'lateral display', 'aggressive'),\n",
    "        (df['behavior'] == 'head-down', 'aggressive'),\n",
    "        (df['behavior'] == 'tailbeat', 'aggressive'), \n",
    "        (df['behavior'] == 'lunging', 'aggressive'),\n",
    "        (df['behavior'] == 'head shake', 'aggressive'),\n",
    "        (df['behavior'] == 'aggressive posture', 'aggressive'), \n",
    "        (df['behavior'] == 'puffed throat', 'aggressive'),\n",
    "        (df['behavior'] == 'sand spitting', 'aggressive'),\n",
    "        (df['behavior'] == 'lunging/shooting out', 'aggressive'),\n",
    "        #non-aggressive/social\n",
    "        (df['behavior'] == 'quivering', 'non-aggressive/social'),\n",
    "        (df['behavior'] == 'soft touch', 'non-aggressive/social'),\n",
    "        (df['behavior'] == 'following', 'non-aggressive/social'),\n",
    "        (df['behavior'] == 'group meeting', 'non-aggressive/social'),\n",
    "        (df['behavior'] == 'parralel swim', 'non-aggressive/social'),\n",
    "        #submissive\n",
    "        (df['behavior'] == 'flee or chased', 'submissive'),\n",
    "        (df['behavior'] == 'bitten', 'submissive'),\n",
    "        (df['behavior'] == 'submissive display', 'submissive'),\n",
    "        #maintenance\n",
    "        (df['behavior'] == 'feed', 'maintenance'),\n",
    "        (df['behavior'] == 'swim', 'maintenance'),\n",
    "        (df['behavior'] == 'still', 'maintenance'),\n",
    "        (df['behavior'] == 'darting', 'maintenance'),\n",
    "        (df['behavior'] == 'yawn', 'maintenance'),\n",
    "        (df['behavior'] == 'scraping', 'maintenance'),\n",
    "        #workload\n",
    "        (df['behavior'] == 'digging', 'workload'),\n",
    "        (df['behavior'] == 'hover', 'workload'),\n",
    "        (df['behavior'] == 'carrying', 'workload'),\n",
    "\n",
    "    ]\n",
    "    condlist = [item[0] for item in _map]\n",
    "    choicelist = [item[1] for item in _map]\n",
    "    \n",
    "    #add behavioral category if not present\n",
    "    df['behavioral_category'] = np.where(df['behavioral_category'].isnull(), np.select(condlist, choicelist, default='unclassified'), df['behavioral_category'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_download_link(filename, title = \"Click here to download: \"):  \n",
    "    data = open(filename, \"rb\").read()\n",
    "    b64 = base64.b64encode(data)\n",
    "    payload = b64.decode()\n",
    "    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "    html = html.format(payload=payload,title=title+f' {filename}',filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "def edit_download_html(htmlWidget, filename, title = \"Click here to download: \"):\n",
    "    \n",
    "    # Change widget html temperarily to a font-awesome spinner\n",
    "    htmlWidget.value = \"<i class=\\\"fa fa-spinner fa-spin fa-2x fa-fw\\\"></i><span class=\\\"sr-only\\\">Loading...</span>\"\n",
    "    \n",
    "    # Process raw data\n",
    "    data = open(filename, \"rb\").read()\n",
    "    b64 = base64.b64encode(data)\n",
    "    payload = b64.decode()\n",
    "    \n",
    "    # Create and assign html to widget\n",
    "    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "    htmlWidget.value = html.format(payload = payload, title = title+filename, filename = filename)\n",
    "    os.remove(filename)\n",
    "    if filename[-1] == 'g':\n",
    "        os.remove(filename.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, behavior, show_avg, show_grid):\n",
    "    \"\"\"\n",
    "    Plot single behaviors or behavioral categories.\n",
    "    \n",
    "    `Required`\n",
    "    :param df: Dataframe containing the behavior data\n",
    "    :param behavior: The single behavior or behavioral category to plot\n",
    "    \n",
    "    `Optional`\n",
    "    :param show_avg: display average line\n",
    "    :param show_grid: display grid\n",
    "    \"\"\"\n",
    "    \n",
    "    #get fish ids and initial empty figure for the plot\n",
    "    fish_ids = get_fish_ids(df)\n",
    "    fig = plt.figure(figsize=(9,7))\n",
    "    average = pd.DataFrame()\n",
    "    highest_plot = 0\n",
    "    \n",
    "    #loop over all fish_ids and plot their amount of selected interactions \n",
    "    for fish in fish_ids:\n",
    "        fish_df = df[df.subject == fish] \n",
    "        if 'behavioral_category' in df:\n",
    "            categories = fish_df[fish_df.behavioral_category == behavior]\n",
    "            behaviors = fish_df[fish_df.behavior == behavior]\n",
    "            fish_df = categories.append(behaviors)\n",
    "        else:\n",
    "            fish_df = fish_df[fish_df.behavior == behavior]\n",
    "        if(len(fish_df)+1>highest_plot):\n",
    "            highest_plot = len(fish_df)+1\n",
    "        sum_of_rows = range(1,len(fish_df)+1)\n",
    "        plt.plot(fish_df.time, sum_of_rows, label=fish)   \n",
    "    #reset colour cycle \n",
    "    plt.gca().set_prop_cycle(None)\n",
    "    \n",
    "    #loop over all fish ids and make a dotted line to the end if the fish is not doing any new \n",
    "    #behaviors but some other fish are or some time is left\n",
    "    for fish in fish_ids:\n",
    "        fish_df = df[df.subject == fish]\n",
    "        if 'behavioral_category' in df:\n",
    "            categories = fish_df[fish_df.behavioral_category == behavior]\n",
    "            behaviors = fish_df[fish_df.behavior == behavior]\n",
    "            fish_df = categories.append(behaviors)\n",
    "        else:\n",
    "            fish_df = fish_df[fish_df.behavior == behavior]\n",
    "        plt.plot([fish_df.time.max(),df.time.max()], [len(fish_df),len(fish_df)],':')\n",
    "    plt.gca().set_prop_cycle(None)\n",
    "    \n",
    "    #loop over all fish ids and make the beginning  before the first behavior of the fish\n",
    "    for fish in fish_ids:\n",
    "        fish_df = df[df.subject == fish]\n",
    "        if 'behavioral_category' in df:\n",
    "            categories = fish_df[fish_df.behavioral_category == behavior]\n",
    "            behaviors = fish_df[fish_df.behavior == behavior]\n",
    "            fish_df = categories.append(behaviors)\n",
    "        else:\n",
    "            fish_df = fish_df[fish_df.behavior == behavior]\n",
    "        plt.plot([0,fish_df.time.min()], [0,1],':')\n",
    "       \n",
    "    #plot average\n",
    "    if show_avg:\n",
    "        avg_df = df.copy()\n",
    "        if 'behavioral_category' in avg_df:\n",
    "            categories = avg_df[avg_df.behavioral_category == behavior]\n",
    "            behaviors = avg_df[avg_df.behavior == behavior]\n",
    "            avg_df = categories.append(behaviors)\n",
    "        else: \n",
    "            avg_df = avg_df[avg_df.behavior == behavior]\n",
    "        avg_range = []\n",
    "        value=1/len(fish_ids)\n",
    "        step=1\n",
    "        while (step <= len(avg_df)):\n",
    "            avg_range.append(value)\n",
    "            value+=1/len(fish_ids)\n",
    "            step+=1\n",
    "        #plot from 0 to 1 dotted, main part, and end dotted\n",
    "        plt.plot([0,avg_df.time.min()], [0,avg_range[0]], ':', color=\"black\")\n",
    "        plt.plot(avg_df.time, avg_range, label=\"average\", color=\"black\")\n",
    "        plt.plot([avg_df.time.max(),df.time.max()], [avg_range[-1], avg_range[-1]], ':', color=\"black\")\n",
    "    \n",
    "    #add legend and edge labels\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time\", fontsize=18, labelpad=10)\n",
    "    plt.ylabel(\"|\" + str(behavior) + \"|\", fontsize=18, labelpad=10)\n",
    "    \n",
    "    #make frequency of yticks dependent on size of the highest plot\n",
    "    if highest_plot < 11:\n",
    "        yticks = range(0,highest_plot)\n",
    "    elif highest_plot < 26:\n",
    "        yticks = range(0,highest_plot, 2)\n",
    "    elif highest_plot < 51:\n",
    "        yticks = range(0,highest_plot, 5)\n",
    "    elif highest_plot < 101:\n",
    "        yticks = range(0,highest_plot, 10)\n",
    "    elif highest_plot < 201:\n",
    "        yticks = range(0,highest_plot, 20)\n",
    "    else:\n",
    "        yticks = range(0,highest_plot, 50)\n",
    "    plt.yticks(yticks)\n",
    "    \n",
    "    if show_grid:\n",
    "        plt.grid(linestyle='-', linewidth=0.2)\n",
    "    \n",
    "    plt.show()\n",
    "    fig.savefig('images/accumulate_actions_plot.png', bbox_inches='tight')\n",
    "    return plt\n",
    "\n",
    "def create_interaction_network(df, threshold=1):\n",
    "    \"\"\"\n",
    "    Create a network showing the interactions between different fish in the dataset. \n",
    "    An edge is drawn or increased by 1 for each row in the dataframe where 'subject' \n",
    "    and 'modifier_1' refer to the same individuals.\n",
    "    \n",
    "    `Required`\n",
    "    :param df: The dataframe containing the behavior data\n",
    "    \n",
    "    `Optional`\n",
    "    :threshold: Threshold for edges to be displayed \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #remove behavior with no interaction partner and irrelevant data\n",
    "    interactions_df = df[df.modifier_1.notna()]\n",
    "    interactions_df = interactions_df[['subject', 'modifier_1']]\n",
    "    \n",
    "    #create a dataframe for the edges \n",
    "    edges_df = interactions_df.groupby(interactions_df.columns.tolist(), as_index=False).size().to_frame(name='records').reset_index()\n",
    "    \n",
    "    #remove edges below the threshold\n",
    "    edges_df = edges_df[edges_df.records >= threshold]\n",
    "    \n",
    "    #add tuples and records as attributes for the network generation\n",
    "    edges_df['tuples'] = list(zip(edges_df.subject, edges_df.modifier_1))\n",
    "    edge_attributes_label = dict(zip(edges_df.tuples, edges_df.records))\n",
    "    \n",
    "    #change for edge weight\n",
    "    edges_df.records = edges_df.records * 3 / edges_df.records.max()\n",
    "    edge_attributes_weight = dict(zip(edges_df.tuples, edges_df.records))\n",
    "    \n",
    "    #create directed graph with networkx\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges_df.tuples)\n",
    "    \n",
    "    #edge labels\n",
    "    nx.set_edge_attributes(G, edge_attributes_label, name='label')\n",
    "    \n",
    "    #edge weight\n",
    "    nx.set_edge_attributes(G, edge_attributes_weight, name='penwidth')\n",
    "    \n",
    "    #graphviz\n",
    "    G_dot_string = to_pydot(G).to_string()\n",
    "    G_dot = graphviz.Source(G_dot_string)\n",
    "    G_dot.format= 'svg'\n",
    "    G_dot.render('images/interactions.gv', view=False)  \n",
    "    display(HTML('images/interactions.gv.svg'))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c2bb1407bf470ea7e4e6c6534e7d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, description='Behavior', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57713bff2dee4ab4adf6d19816939022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a button to upload a file containing behavior data\n",
    "uploader_bhvr = widgets.FileUpload(description='Behavior', multiple=True)\n",
    "display(uploader_bhvr)\n",
    "out = widgets.Output()\n",
    "display(out)\n",
    "\n",
    "def upload_handler(_):\n",
    "    \"\"\"\n",
    "    Handle File Upload. On success, display metainformation about the dataset and initialize \n",
    "    global dataframe variable df. On failure, give instructions on what is missing or wrong.\n",
    "    \n",
    "    `Required`\n",
    "    :param change: Indicates new file upload\n",
    "    \"\"\"\n",
    "    \n",
    "    global df,df_name\n",
    "    upload_sanitized = False\n",
    "    \n",
    "    #read uploaded file into dataframe, display message if wrong file format\n",
    "    [behavior] = uploader_bhvr.value\n",
    "    try:\n",
    "        df = pd.read_csv(io.BytesIO(uploader_bhvr.value[behavior][\"content\"]))\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_excel(io.BytesIO(uploader_bhvr.value[behavior][\"content\"]))\n",
    "        except: \n",
    "            with out:\n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(\"\"\"File must be of type <em>.csv</em> or <em>.xlsx</em>\"\"\"))\n",
    "                return\n",
    "        \n",
    "    #clean file and display message if required header(s) are missing\n",
    "    try:\n",
    "        df = _clean(df)\n",
    "        upload_sanitized = True\n",
    "    except: \n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            if 'time' not in df.columns:\n",
    "                display(Markdown(\"\"\" Missing column header: <em>Time/time</em> \"\"\"))\n",
    "            if 'subject' not in df.columns:\n",
    "                display(Markdown(\"\"\" Missing column header: <em>Subject/subject</em> \"\"\"))\n",
    "            if 'behavior' not in df.columns:\n",
    "                display(Markdown(\"\"\" Missing column header: <em>Behavior/behavior</em> \"\"\"))\n",
    "            if 'status' not in df.columns:\n",
    "                display(Markdown(\"\"\" Missing column header: <em>Status/status</em> \"\"\"))\n",
    "            \n",
    "    #if data upload and sanitation successful, display information about dataset\n",
    "    if(upload_sanitized):\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            display(Markdown(\"\"\"---\"\"\"))\n",
    "            display(Markdown(\"\"\"#### File name\"\"\"))\n",
    "            df_name = next(iter(uploader_bhvr.value))\n",
    "            print(df_name)\n",
    "            display(Markdown(\"\"\"#### IDs\"\"\"))\n",
    "            print(get_fish_ids(df))\n",
    "            display(Markdown(\"\"\"#### Behavioral categories\"\"\"))\n",
    "            print(df.behavioral_category.unique())\n",
    "            display(Markdown(\"\"\"#### Behaviors\"\"\"))\n",
    "            print(df.behavior.unique())\n",
    "            display(Markdown(\"\"\"---\"\"\"))\n",
    "    \n",
    "            #display data plot\n",
    "            display(Markdown(\"\"\"## Data plot \n",
    "            \\n - <strong>Usage</strong>: Double-click in the behavior-field, then use Up/Down-keys\"\"\"))\n",
    "            data_plot = interactive(plot_data, \n",
    "                                    df=fixed(df), \n",
    "                                    behavior = np.concatenate((df.behavioral_category.unique(),df.behavior.unique()), axis=0),\n",
    "                                    show_avg = True,\n",
    "                                    show_grid = True)\n",
    "            display(data_plot)\n",
    "            \n",
    "            #display  interaction network\n",
    "            display(Markdown(\"\"\"## Interaction network \n",
    "            \\n - <strong>Usage</strong>: Optionally choose threshold for displayed edges\"\"\"))\n",
    "            interaction_network = interactive(create_interaction_network, \n",
    "                                              df = fixed(df), \n",
    "                                              threshold=(1,100,1))\n",
    "            display(interaction_network)\n",
    "            \n",
    "    return\n",
    "\n",
    "#connect on_upload_change function to file upload widget by using its internal counter\n",
    "uploader_bhvr.observe(upload_handler, names='_counter')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Transition probability network \n",
       "\n",
       " - <strong>Usage</strong>: Choose parameters, click <em>Execute</em>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2219c7ae984a6b865300bf317e3737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(VBox(children=(ToggleButtons(options=('behavior', 'behavioral_category'), value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#buttons for the transition probability network\n",
    "display(Markdown(\"\"\"---\"\"\"))\n",
    "display(Markdown(\"\"\"## Transition probability network \n",
    "\\n - <strong>Usage</strong>: Choose parameters, click <em>Execute</em>\"\"\"))\n",
    "\n",
    "data_button = widgets.ToggleButtons(\n",
    "    options=['behavior', 'behavioral_category'],\n",
    "    value='behavior',\n",
    ")\n",
    "\n",
    "min_count_button = widgets.FloatText(\n",
    "    value=0.0,\n",
    "    description='Min_edge_val',\n",
    ")\n",
    "\n",
    "rmv_id_button = widgets.Text(\n",
    "    value='',\n",
    "    description='Delete ID'\n",
    ")\n",
    "\n",
    "add_id_button = widgets.Text(\n",
    "    value='',\n",
    "    description='Add ID'\n",
    ")\n",
    "\n",
    "rmv_bhvr_button = widgets.Text(\n",
    "    value='',\n",
    "    description='Delete node'\n",
    ")\n",
    "\n",
    "add_bhvr_button = widgets.Text(\n",
    "    value='',\n",
    "    description='Add node'\n",
    ")\n",
    "\n",
    "status_button = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Show status',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "normalized_button = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Normalize',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "hue_slider = widgets.IntSlider(\n",
    "    value=140,\n",
    "    min=1,\n",
    "    max=360,\n",
    "    step=1,\n",
    "    description='Color hue:'\n",
    ")\n",
    "\n",
    "colour_button = widgets.ToggleButtons(\n",
    "    options=['amount', 'total_time', 'avg_time'],\n",
    "    value='avg_time',\n",
    "    description='Colour')\n",
    "    \n",
    "size_button = widgets.ToggleButtons(\n",
    "    options=['amount', 'total_time', 'avg_time'],\n",
    "    value='total_time',\n",
    "    description='Size')\n",
    "    \n",
    "sort_button = widgets.ToggleButtons(\n",
    "    options=['amount', 'total_time', 'avg_time'],\n",
    "    value='total_time',\n",
    "    description='Sort by')\n",
    "\n",
    "label_button = widgets.ToggleButtons(\n",
    "    options=['amount', 'total_time', 'avg_time', ' - '],\n",
    "    value=' - ',\n",
    "    description='Label')\n",
    "\n",
    "execute_network_button = widgets.Button(\n",
    "    description = 'Execute'\n",
    ")\n",
    "\n",
    "download_gpickle = widgets.HTML(value = '')\n",
    "download_svg = widgets.HTML(value = '')\n",
    "\n",
    "\n",
    "#tabs for organizing the buttons of the transition probabilty network\n",
    "\n",
    "tab1 = VBox(children=[data_button,\n",
    "                      HBox(children=[normalized_button, status_button,]),\n",
    "                     ])\n",
    "\n",
    "tab2 = VBox(children=[size_button,\n",
    "                      label_button,\n",
    "                      colour_button,\n",
    "                      hue_slider,\n",
    "                     ])\n",
    "\n",
    "tab3 = VBox(children=[rmv_bhvr_button,\n",
    "                      add_bhvr_button,\n",
    "                      rmv_id_button,\n",
    "                      add_id_button,\n",
    "                      min_count_button,\n",
    "                     ])\n",
    "\n",
    "tab4 = VBox(children=[download_gpickle,\n",
    "                      download_svg,\n",
    "                     ])\n",
    "\n",
    "tab = widgets.Tab(children=[tab1, tab2, tab3, tab4])\n",
    "tab.set_title(0, 'network')\n",
    "tab.set_title(1, 'nodes')\n",
    "tab.set_title(2, 'thin out')\n",
    "tab.set_title(3, 'download')\n",
    "\n",
    "network_output = widgets.Output()\n",
    "\n",
    "transition_probability_network = VBox(children=[tab, execute_network_button, network_output])\n",
    "\n",
    "    \n",
    "@execute_network_button.on_click\n",
    "def execute_network(_):\n",
    "    with network_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            create_behavior_cycle()\n",
    "        except:\n",
    "            display(Markdown(\"\"\"#### Error: No uploaded file detected - Please try again after successful file upload\"\"\"))\n",
    "    #clear input fields\n",
    "    rmv_id_button.value = ''\n",
    "    add_id_button.value = ''\n",
    "    rmv_bhvr_button.value = ''\n",
    "    add_bhvr_button.value = ''\n",
    "    \n",
    "\n",
    "    \n",
    "display(transition_probability_network)\n",
    "\n",
    "remove_list_cat = []\n",
    "remove_list = []\n",
    "remove_id_list = []\n",
    "def create_behavior_cycle():\n",
    "    \"\"\"Input parameters are the behavior file, the specification if the user wants to see the behaviors itself \n",
    "    or the behavior cycle of the behavioral categories and the minimal count for a edge to be displayed. \n",
    "    This cycle is calculated by splitting the boris-file for each fish and then increasing the edge count for each \n",
    "    successing behavior. In the end, the edge count is normalized in [0,1] for each node where edges come from \n",
    "    so we have kind of a probability of which behavior follows which behavior\"\"\"\n",
    "    global df, df_name, remove_list_cat, remove_list, remove_id_list\n",
    "    local_df = df\n",
    "    \n",
    "    #display tab\n",
    "    data = data_button.value\n",
    "    with_status = status_button.value\n",
    "    normalized = normalized_button.value\n",
    "    \n",
    "    #reduce tab\n",
    "    min_count = min_count_button.value\n",
    "    rmv_id = rmv_id_button.value\n",
    "    add_id = add_id_button.value\n",
    "    rmv_bhvr = rmv_bhvr_button.value\n",
    "    add_bhvr = add_bhvr_button.value\n",
    "    \n",
    "    #node tab\n",
    "    hue = hue_slider.value\n",
    "    node_colour= colour_button.value\n",
    "    node_size= size_button.value\n",
    "    node_label= label_button.value\n",
    "    sort_by= sort_button.value\n",
    "    \n",
    "    fish_ids = get_fish_ids(df)\n",
    "    successor_list = []\n",
    "    #prepare dataframe with user input\n",
    "    #first check if the user wants so see the behaviors or the behavioral categories\n",
    "    if data == 'behavioral_category':\n",
    "        #reset list of removed behaviors\n",
    "        remove_list.clear()\n",
    "        local_df['chosen_data'] = local_df['behavioral_category']\n",
    "        \n",
    "        #print unique behavioral categories\n",
    "        display(Markdown(\"\"\"#### All behavioral categories: \\n\"\"\"))\n",
    "        print(df.chosen_data.unique())\n",
    "        \n",
    "        #remove and add behavioral categories\n",
    "        if rmv_bhvr:\n",
    "            remove_us = rmv_bhvr.split('\\'')\n",
    "            for x in remove_us:\n",
    "                if x in local_df.chosen_data.unique() and (len(remove_list_cat)+1 < len(local_df.chosen_data.unique())):\n",
    "                    remove_list_cat.append(x)\n",
    "        \n",
    "        if add_bhvr:\n",
    "            add_us = add_bhvr.split('\\'')\n",
    "            for x in add_us:\n",
    "                if x in df.behavioral_category.unique() and x in remove_list_cat:\n",
    "                    remove_list_cat.remove(x)\n",
    "        \n",
    "        #display removed behaviors and create new reduced dataframe\n",
    "        if remove_list_cat:\n",
    "            display(Markdown(\"\"\"#### Removed behavioral categories: \\n\"\"\"))\n",
    "            print(set(remove_list_cat))\n",
    "            for x in remove_list_cat:\n",
    "                local_df = local_df.drop(local_df[local_df.behavioral_category == x].index)\n",
    "    else:\n",
    "        #reset list of removed behavioral categories\n",
    "        remove_list_cat.clear()\n",
    "        local_df['chosen_data'] = local_df['behavior']\n",
    "        #print all behaviors\n",
    "        display(Markdown(\"\"\"#### All behaviors:\"\"\"))\n",
    "        print(local_df.chosen_data.unique())\n",
    "       \n",
    "        #add and remove behaviors\n",
    "        if rmv_bhvr:\n",
    "            remove_us = rmv_bhvr.split('\\'')\n",
    "            for x in remove_us:\n",
    "                if x in local_df.chosen_data.unique() and (len(remove_list)+1 < len(local_df.chosen_data.unique())):\n",
    "                    remove_list.append(x)            \n",
    "        if add_bhvr:\n",
    "            add_us = add_bhvr.split('\\'')\n",
    "            for x in add_us:\n",
    "                if x in df.chosen_data.unique() and x in remove_list:\n",
    "                    remove_list.remove(x)  \n",
    "        if remove_list:\n",
    "            display(Markdown(\"\"\"#### Removed behavior: \\n\"\"\"))\n",
    "            print(set(remove_list))\n",
    "            for x in remove_list:\n",
    "                local_df = local_df.drop(df[df.chosen_data == x].index)\n",
    "   \n",
    "    #remove IDs\n",
    "    if rmv_id:\n",
    "        remove_ids = rmv_id.split('\\'')\n",
    "        for x in remove_ids:\n",
    "            if x in fish_ids and len(remove_id_list)+1 < len(fish_ids):\n",
    "                remove_id_list.append(x)\n",
    "    if add_id:\n",
    "        add_ids = add_id.split('\\'')\n",
    "        for x in add_ids:\n",
    "            if (x in fish_ids or x in df.modifier_1.unique()) and x in remove_id_list:\n",
    "                remove_id_list.remove(x)\n",
    "    if remove_id_list:\n",
    "        display(Markdown(\"\"\"#### Removed IDs: \\n\"\"\"))\n",
    "        print(set(remove_id_list))\n",
    "    fish_ids_after_removal = [x for x in fish_ids if x not in remove_id_list]\n",
    "    \n",
    "    display(Markdown(\"\"\" --- \"\"\"))\n",
    "    \n",
    "    #loop through dataframe for each fish and add behavior and successor\n",
    "    for fish in fish_ids_after_removal:\n",
    "        id_frame = local_df[local_df.subject == fish]  \n",
    "        if not (with_status):\n",
    "            id_frame = id_frame.drop(id_frame[id_frame.status == 'STOP'].index)\n",
    "        i=0\n",
    "        k=i+1\n",
    "        while i < len(id_frame)-1:\n",
    "            successor_list.append((id_frame.chosen_data.iloc[i], id_frame.status.iloc[i], id_frame.chosen_data.iloc[k], id_frame.status.iloc[k]))\n",
    "            k+=1\n",
    "            i+=1\n",
    "    #lets make an edgelist with behavior and successor\n",
    "    successor_df = pd.DataFrame(successor_list, columns=['action_1', 'status_1', 'action_2', 'status_2'])\n",
    "    if (with_status):\n",
    "        successor_df['action_1'] = successor_df['action_1'] + ' ' + successor_df['status_1']\n",
    "        successor_df['action_2'] = successor_df['action_2'] + ' ' + successor_df['status_2']\n",
    "    else:\n",
    "        successor_df = successor_df.replace(to_replace=\"POINT\", value=\"\")\n",
    "    \n",
    "    successor_df['tuples'] = list(zip(successor_df.action_1, successor_df.action_2))\n",
    "    successor_df = successor_df.groupby(successor_df.columns.tolist(), as_index=False).size().to_frame(name='records').reset_index()\n",
    "    \n",
    "    #normalize the records in [0,1] so that all together are 1 for each action\n",
    "    behavior_ids = successor_df.action_1.unique().tolist()\n",
    "    edges_df = pd.DataFrame()\n",
    "    for action in behavior_ids:\n",
    "        action_frame = successor_df[successor_df.action_1 == action]\n",
    "        if(normalized):    \n",
    "            sum_of_successors = action_frame.records.sum()\n",
    "            action_frame['normalized'] = action_frame.records.div(sum_of_successors).round(2)\n",
    "        edges_df = edges_df.append(action_frame)   \n",
    "    \n",
    "\n",
    "    #erase edges below min_count\n",
    "    try:\n",
    "        if(normalized and min_count):\n",
    "            edges_df = edges_df[edges_df.normalized > float(min_count)]\n",
    "        elif not normalized and min_count:    \n",
    "            edges_df = edges_df[edges_df.records > float(min_count)]\n",
    "    except: display(Markdown(\"\"\"#### min_count has to be a positive real number. No edges were removed.\\n\"\"\"))\n",
    "    \n",
    "    # add average and total time\n",
    "    times_list = get_total_and_avg_time(df, fish_ids_after_removal)\n",
    "    times_df = pd.DataFrame(times_list, columns=['action_1', 'total_time', 'avg_time'])\n",
    "    \n",
    "    #work on the nodes(behaviors) of the graph so we can later set node-attributes for graphviz\n",
    "    nodes_df = edges_df[['action_1', 'records']]\n",
    "    nodes_df = edges_df.groupby('action_1')['records'].sum().to_frame(name='records').reset_index()\n",
    "    nodes_df = pd.merge(times_df, nodes_df, on='action_1', how='outer')\n",
    "    nodes_df.columns = ['node', 'total_time', 'avg_time', 'record']\n",
    "    \n",
    "    #if a behavior occurs only once/ as last behavior maybe of an animal it is not counted\n",
    "    if not (with_status):\n",
    "        nodes_df.record = nodes_df.record.fillna(1)\n",
    "    #round results\n",
    "    nodes_df.total_time = nodes_df.total_time.round(2)\n",
    "    nodes_df.avg_time = nodes_df.avg_time.round(2)\n",
    "    \n",
    "    #merge nodes with amount and times in the dataframe for the tuples so \n",
    "    #they can be displayed inside the node as label\n",
    "    labels_1 = nodes_df.copy()\n",
    "    labels_1.columns = ['action_1', 'total_time_1', 'avg_time_1', 'record_1']\n",
    "    edges_df = pd.merge(edges_df, labels_1, on='action_1', how='left')\n",
    "    labels_2 = nodes_df.copy()\n",
    "    labels_2.columns = ['action_2', 'total_time_2', 'avg_time_2', 'record_2']\n",
    "    edges_df = pd.merge(edges_df, labels_2, on='action_2', how='left') \n",
    "    \n",
    "    if(node_label == 'amount'):\n",
    "        edges_df['action_1'] = edges_df['action_1'] + \" - \" + edges_df['record_1'].astype(str)\n",
    "        edges_df['action_2'] = edges_df['action_2'] + \" - \" + edges_df['record_2'].astype(str)\n",
    "        edges_df['tuples'] = list(zip(edges_df['action_1'],edges_df['action_2']))\n",
    "        nodes_df['node'] = nodes_df['node'] + \" - \" + nodes_df['record'].astype(str)\n",
    "    elif(node_label == 'total_time'):\n",
    "        edges_df['action_1'] = edges_df['action_1'] + \" - \" + edges_df['total_time_1'].astype(str)\n",
    "        edges_df['action_2'] = edges_df['action_2'] + \" - \" + edges_df['total_time_2'].astype(str)\n",
    "        edges_df['tuples'] = list(zip(edges_df['action_1'],edges_df['action_2']))\n",
    "        nodes_df['node'] = nodes_df['node'] + \" - \" + nodes_df['total_time'].astype(str)\n",
    "    elif(node_label == 'avg_time'):\n",
    "        edges_df['action_1'] = edges_df['action_1'] + \" - \" + edges_df['avg_time_1'].astype(str)\n",
    "        edges_df['action_2'] = edges_df['action_2'] + \" - \" + edges_df['avg_time_2'].astype(str)\n",
    "        edges_df['tuples'] = list(zip(edges_df['action_1'],edges_df['action_2']))\n",
    "        nodes_df['node'] = nodes_df['node'] + \" - \" + nodes_df['avg_time'].astype(str)\n",
    "    \n",
    "    if(sort_by == 'amount'):\n",
    "        nodes_df = nodes_df.sort_values(by='record', ascending=False)\n",
    "    elif(sort_by == 'total_time'):\n",
    "        nodes_df = nodes_df.sort_values(by='total_time', ascending=False)\n",
    "    else:\n",
    "        nodes_df = nodes_df.sort_values(by='avg_time', ascending=False)\n",
    "    \n",
    "    # print behavior nodes and amount\n",
    "    display(HTML(nodes_df.to_html(index=False)))\n",
    "    display(Markdown(\"\"\" ---\"\"\"))\n",
    "    \n",
    "    \n",
    "    #logarithmic normalization of record, avg_time and total_time \n",
    "    nodes_df.record = (np.log(nodes_df.record)-np.log(nodes_df.record.min()))/(np.log(nodes_df.record.max())-np.log(nodes_df.record.min()))\n",
    "    nodes_df.total_time = nodes_df.total_time+1\n",
    "    nodes_df.total_time = (np.log(nodes_df.total_time)-np.log(nodes_df.total_time.min()))/(np.log(nodes_df.total_time.max())-np.log(nodes_df.total_time.min()))\n",
    "    nodes_df.avg_time = nodes_df.avg_time+1\n",
    "    nodes_df.avg_time = (np.log(nodes_df.avg_time)-np.log(nodes_df.avg_time.min()))/(np.log(nodes_df.avg_time.max())-np.log(nodes_df.avg_time.min()))\n",
    "        \n",
    "    #node sizes dependent on user input and then a dictionary \n",
    "    #for node height and width is created to give it to graphviz\n",
    "    if(node_size == 'amount'):\n",
    "        nodes_width = dict(zip(nodes_df.node, nodes_df.record*3))\n",
    "        nodes_height = dict(zip(nodes_df.node, nodes_df.record*1.4))\n",
    "    elif (node_size == 'total_time'):\n",
    "        nodes_width = dict(zip(nodes_df.node, nodes_df.total_time*3))\n",
    "        nodes_height = dict(zip(nodes_df.node, nodes_df.total_time*1.4))\n",
    "    elif (node_size == 'avg_time'):\n",
    "        nodes_width = dict(zip(nodes_df.node, nodes_df.avg_time*3))\n",
    "        nodes_height = dict(zip(nodes_df.node, nodes_df.avg_time*1.4))\n",
    "        \n",
    "    #node colour dependent on user input, values are normalized with np.log and then a dictionary\n",
    "    #for node colour is created to give it to graphviz later\n",
    "    hue = hue/360\n",
    "    if(node_colour == 'amount'):\n",
    "        nodes_df['colour'] = str(hue)+\" \"+ nodes_df['record'].astype(str) + \" 1\"\n",
    "        nodes_colour = dict(zip(nodes_df.node, nodes_df.colour))\n",
    "    elif (node_colour == 'total_time'):\n",
    "        nodes_df['colour'] = str(hue)+\" \"+ nodes_df['total_time'].astype(str) + \" 1\"\n",
    "        nodes_colour = dict(zip(nodes_df.node, nodes_df.colour))\n",
    "    elif (node_colour == 'avg_time'):\n",
    "        nodes_df['colour'] = str(hue)+\" \"+ nodes_df['avg_time'].astype(str) + \" 1\"\n",
    "        nodes_colour = dict(zip(nodes_df.node, nodes_df.colour))\n",
    "    \n",
    "    #create directed graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges_df.tuples)\n",
    "    \n",
    "    #create label and weight for edges\n",
    "    if(normalized):\n",
    "        edge_attributes_label = dict(zip(edges_df.tuples, edges_df.normalized))\n",
    "        edges_df.normalized = edges_df.normalized * 3\n",
    "        edge_attributes_weight = dict(zip(edges_df.tuples, edges_df.normalized))\n",
    "    else:\n",
    "        edge_attributes_label = dict(zip(edges_df.tuples, edges_df.records))\n",
    "        #normalize logarithmic\n",
    "        edges_df.records = (np.log(edges_df.records)-np.log(edges_df.records.min()))/(np.log(edges_df.records.max())-np.log(edges_df.records.min()))\n",
    "        edges_df.records = edges_df.records + 0.1\n",
    "        edge_attributes_weight = dict(zip(edges_df.tuples, edges_df.records/edges_df.records.max()))\n",
    "    \n",
    "    #set edge attributes\n",
    "    nx.set_edge_attributes(G, edge_attributes_weight, name='penwidth')\n",
    "    nx.set_edge_attributes(G, edge_attributes_label, name='label')\n",
    "    \n",
    "    #set node attributes\n",
    "    nx.set_node_attributes(G, nodes_width, name='width')\n",
    "    nx.set_node_attributes(G, nodes_height, name='height')\n",
    "    if not (with_status):\n",
    "        nx.set_node_attributes(G, nodes_colour, name='fillcolor')\n",
    "    nx.set_node_attributes(G, 'filled', name='style')\n",
    "    nx.set_node_attributes(G, \"what should &#013; be here\", name='tooltip')\n",
    "    \n",
    "    #use time to name the files\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    #get a good filename for the newly generated network download, consisting of the users name + date\n",
    "    filename = df_name\n",
    "    if filename.endswith('.csv'):\n",
    "        filename = filename[:-4]\n",
    "    elif filename.endswith('.xlsx'):\n",
    "        filename = filename[:-5]\n",
    "    #filename = filename + \"-\"+ str(now.year) +\"_\"+ str(now.month) +\"_\"+ str(now.day) \n",
    "    filename = filename.replace(\".\",\"_\")\n",
    "    #graphviz\n",
    "    G_dot_string = to_pydot(G).to_string()\n",
    "    G_dot = graphviz.Source(G_dot_string)\n",
    "    G_dot.format= 'svg'\n",
    "    G_dot.render(filename, view=False)  \n",
    "    display(HTML(filename + '.svg'))\n",
    "    \n",
    "    #make download available\n",
    "    nx.write_gpickle(G, filename + \".gpickle\")\n",
    "    download_link = create_download_link(filename + \".gpickle\", title = \"Download for network comparison:  \")\n",
    "    display(download_link)\n",
    "    edit_download_html(download_gpickle, filename + \".gpickle\", title = \"Download for network comparison: \")\n",
    "    edit_download_html(download_svg, filename + '.svg', title = \"Download svg image: \")\n",
    "\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " \n",
       "## Network comparison \n",
       "- <strong>Usage</strong>: Upload previously downloaded transition probability networks \n",
       " by clicking the <em>Network</em>-buttons\n",
       "\n",
       "- <strong>Info</strong>: This functionality is still in progress, do not expect much output \n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe76ba66177452882d3b82548a31dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, description='Network-1', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdb3a211de34ec7bffd3edebc089d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, description='Network-2', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79485165a79c4813bb8042c082a8265b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare different networks previously uploaded\n",
    "display(Markdown(\"\"\"---\"\"\"))\n",
    "display(Markdown(\n",
    "\"\"\" \n",
    "## Network comparison \n",
    "- <strong>Usage</strong>: Upload previously downloaded transition probability networks \\n by clicking the <em>Network</em>-buttons\\n\n",
    "- <strong>Info</strong>: This functionality is still in progress, do not expect much output \\n\n",
    "---\n",
    "\"\"\"))\n",
    "\n",
    "#upload buttons and output for the network comparison\n",
    "uploader_network1 = widgets.FileUpload(description='Network-1', multiple=True)\n",
    "display(uploader_network1)\n",
    "uploader_network2 = widgets.FileUpload(description='Network-2', multiple=True)\n",
    "display(uploader_network2)\n",
    "out_comp = widgets.Output()\n",
    "display(out_comp)\n",
    "  \n",
    "\n",
    "def comp_upload_handler1(_):\n",
    "    \"\"\"\n",
    "    Handle file upload of uploader_network1, gets triggered as file upload changes\n",
    "    \n",
    "    `Required` \n",
    "    :param _: A needed filler for it to work\n",
    "    \"\"\"\n",
    "    global G1,G2\n",
    "    try:\n",
    "        [network1] = uploader_network1.value\n",
    "        G1 = nx.read_gpickle(io.BytesIO(uploader_network1.value[network1][\"content\"]))\n",
    "        G1.graph['name'] = next(iter(uploader_network1.value)).split(\".\", 1)[0]\n",
    "        \n",
    "        with out_comp:\n",
    "            clear_output(wait=True)\n",
    "            display(Markdown(\"\"\"First network successfully uploaded : \"\"\" + G1.graph['name'] ))\n",
    "            \n",
    "            #now the real network comparison\n",
    "            if (G1 and G2):\n",
    "                compare_networks(G1,G2)\n",
    "            \n",
    "    except:\n",
    "        #check for correct file ending\n",
    "        with out_comp:\n",
    "            clear_output(wait=True)\n",
    "            display(Markdown(\"\"\"An error occured: \\n\"\"\"))\n",
    "            if \".gpickle\" not in next(iter(uploader_network1.value)):\n",
    "                display(Markdown(\"\"\"File must be of type <em>.gpickle</em>\"\"\"))\n",
    "    \n",
    "    return\n",
    "\n",
    "def comp_upload_handler2(_):\n",
    "    \"\"\"\n",
    "    Handle file upload of uploader_network2, gets triggered as file upload changes\n",
    "    \n",
    "    `Required` \n",
    "    :param _: A needed filler for it to work\n",
    "    \"\"\"\n",
    "    global G1,G2\n",
    "    \n",
    "    try:\n",
    "        [network2] = uploader_network2.value\n",
    "        G2 = nx.read_gpickle(io.BytesIO(uploader_network2.value[network2][\"content\"]))\n",
    "        G2.graph['name'] = next(iter(uploader_network2.value)).split(\".\", 1)[0]\n",
    "        \n",
    "        with out_comp:\n",
    "            clear_output(wait=True)\n",
    "            display(Markdown(\"\"\"Second network successfully uploaded : \"\"\" + G2.graph['name'] ))\n",
    "            \n",
    "            #now the real network comparison\n",
    "            if (G1 and G2):\n",
    "                clear_output(wait=True)\n",
    "                compare_networks(G1,G2)\n",
    "            \n",
    "    except:\n",
    "        #check for correct file ending\n",
    "        with out_comp:\n",
    "            clear_output(wait=True)\n",
    "            display(Markdown(\"\"\"An error occured: \\n\"\"\"))\n",
    "            if \".gpickle\" not in next(iter(uploader_network2.value)):\n",
    "                display(Markdown(\"\"\"File must be of type <em>.gpickle</em>\"\"\"))\n",
    "    \n",
    "    return\n",
    "    \n",
    "#connect on_upload_change function to file upload widget by using its internal counter\n",
    "uploader_network1.observe(comp_upload_handler1, names='_counter') \n",
    "uploader_network2.observe(comp_upload_handler2, names='_counter') \n",
    "\n",
    "def avg_degree_centrality(G):\n",
    "    #calculate average degree centrality\n",
    "    values = nx.degree_centrality(G).values()\n",
    "    avg_degree_centrality = sum(values)/len(values)\n",
    "    return avg_degree_centrality\n",
    "\n",
    "def select_k(spectrum, minimum_energy = 0.9):\n",
    "    running_total = 0.0\n",
    "    total = sum(spectrum)\n",
    "    if total == 0.0:\n",
    "        return len(spectrum)\n",
    "    for i in range(len(spectrum)):\n",
    "        running_total += spectrum[i]\n",
    "        if running_total / total >= minimum_energy:\n",
    "            return i + 1\n",
    "    return len(spectrum)\n",
    "\n",
    "\n",
    "def eigenvector_similarity(G1, G2):\n",
    "    \"\"\"\n",
    "    Calculate the eigenvector similarity for the undirected equivalents of the input Graphs.\n",
    "    \n",
    "    `Required`\n",
    "    :param G1: the first graph to be compared\n",
    "    :param G2: the second graph to be compared\n",
    "    \"\"\"\n",
    "    display(Markdown(\"\"\"---\"\"\"))\n",
    "    display(Markdown(\"\"\"Eigenvector Similarity: \\n \n",
    "    Calculate the Laplacian eigenvalues for the adjacency matrices of each of the graphs. \n",
    "    For each graph, find the smallest k such that the sum of the k largest eigenvalues constitutes at least 90% of the sum of all of the eigenvalues. \n",
    "    If the values of k are different between the two graphs, then use the smaller one. \n",
    "    The similarity metric is then the sum of the squared differences between the largest k eigenvalues between the graphs. \n",
    "    This will produce a similarity metric in the range [0, ∞), where values closer to zero are more similar. Neither edge direction nor edge weigth are taken in account.\"\"\"))\n",
    "    display(Markdown(\"\"\"<sub><em>Source: ESultanik on stackoverflow, this is his post: https://stackoverflow.com/a/27303476</em> </sub>\"\"\"))\n",
    "\n",
    "    \n",
    "    G1 = G1.to_undirected()\n",
    "    G2 = G2.to_undirected()\n",
    "    #https://stackoverflow.com/questions/12122021/python-implementation-of-a-graph-similarity-grading-algorithm\n",
    "    laplacian1 = nx.spectrum.laplacian_spectrum(G1)\n",
    "    laplacian2 = nx.spectrum.laplacian_spectrum(G2)\n",
    "\n",
    "    k1 = select_k(laplacian1)\n",
    "    k2 = select_k(laplacian2)\n",
    "    k = min(k1, k2)\n",
    "\n",
    "    similarity = sum((laplacian1[:k] - laplacian2[:k])**2)\n",
    "    \n",
    "    display(Markdown(\"\"\"<strong>Similarity score:</strong> \"\"\" + str(similarity)))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def union_graph(G,H):\n",
    "    \"\"\"\n",
    "    Calculate and display an graph showing the intersections of the union of the two graphs\n",
    "    \n",
    "    `Required`\n",
    "    :param G1: the first graph to be compared\n",
    "    :param G2: the second graph to be compared\n",
    "    \"\"\"\n",
    "    \n",
    "    GH =nx.compose(G,H)\n",
    "    \n",
    "    # set edge colors\n",
    "    edge_colors = dict()\n",
    "    edge_labels = dict()\n",
    "    edge_width = dict()\n",
    "    for edge in GH.edges():\n",
    "        edge_width[edge] = 0.9\n",
    "        if G.has_edge(*edge):\n",
    "            if H.has_edge(*edge):\n",
    "                edge_colors[edge] = 'azure3'\n",
    "                edge_labels[edge] = ''\n",
    "                edge_width[edge] = 0.8\n",
    "                continue\n",
    "            edge_colors[edge] = 'magenta'\n",
    "        elif H.has_edge(*edge):\n",
    "            edge_colors[edge] = 'chartreuse1'\n",
    "\n",
    "    # set node colors\n",
    "    G_nodes = set(G.nodes())\n",
    "    H_nodes = set(H.nodes())\n",
    "    node_colors = []\n",
    "    for node in GH.nodes():\n",
    "        if node in G_nodes:\n",
    "            if node in H_nodes:\n",
    "                node_colors.append('azure3')\n",
    "                continue\n",
    "            node_colors.append('magenta')\n",
    "        if node in H_nodes:\n",
    "            node_colors.append('chartreuse1')\n",
    "            \n",
    "    node_colors_dict = dict(zip(GH.nodes(), node_colors))\n",
    "            \n",
    "    #graphviz draw\n",
    "    nx.set_edge_attributes(GH, edge_colors, name='color')\n",
    "    nx.set_edge_attributes(GH, edge_labels, name='label')\n",
    "    nx.set_edge_attributes(GH, edge_width, name='penwidth')\n",
    "    nx.set_node_attributes(GH, node_colors_dict, name='fillcolor')\n",
    "    nx.set_node_attributes(GH, 1.2, name ='width')\n",
    "    nx.set_node_attributes(GH, 0.5, name ='height')\n",
    "    #graphviz\n",
    "    GH_dot_string = to_pydot(GH).to_string()\n",
    "    GH_dot = graphviz.Source(GH_dot_string)\n",
    "    GH_dot.format= 'svg'\n",
    "    GH_dot.render('images/uniongraph', view=False)  \n",
    "    \n",
    "    #display metainformation and graph\n",
    "    display(Markdown(\"\"\"---\"\"\"))\n",
    "    print(colored(G.graph['name'], 'magenta'))\n",
    "    print(colored(H.graph['name'], 'green'))\n",
    "    print(colored(\"Intersecting nodes/edges \\n\", 'grey'))\n",
    "    display(HTML('images/uniongraph.svg'))\n",
    "    \n",
    "    i = set(G.edges()).intersection(H.edges())\n",
    "    distance = round(len(i) / (len(G.edges()) + len(H.edges()) - len(i)),3)    \n",
    "    \n",
    "    display(Markdown(\"\"\"<strong>Ratio of Intersection over Union</strong>: \"\"\" + str(distance)))\n",
    "    \n",
    "    return\n",
    "\n",
    "def jaccard(G1, G2):\n",
    "    \"\"\"\n",
    "    Display intersective nodes and edges of union of graphs. \n",
    "    Calculate and display jaccard distance of the two graphs\n",
    "    \n",
    "    `Required`\n",
    "    :param G1: the first graph to be compared\n",
    "    :param G2: the second graph to be compared\n",
    "    \"\"\"\n",
    "    display(Markdown(\"\"\"---\"\"\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    The Jaccard index, also known as the Jaccard similarity coefficient, \n",
    "    is a statistic used for gauging the similarity and diversity of sample sets. \n",
    "    In this case, the Jaccard coefficient measures similarity between the graphs by dividing the intersection \n",
    "    by the size of the union of the edge sets. \"\"\"))\n",
    "    \n",
    "\n",
    "    i = set(G1.edges()).intersection(G2.edges())\n",
    "    distance = round(len(i) / (len(G1.edges()) + len(G2.edges()) - len(i)),3)    \n",
    "    \n",
    "    display(Markdown(\"\"\"<strong>Ratio of intersection over union of edges:</strong> \"\"\" + str(distance)))\n",
    "\n",
    "    return\n",
    "\n",
    "def graph_edit_distance(G1, G2):\n",
    "    \"\"\"\n",
    "    Calculate and display the graph edit distance\n",
    "    \n",
    "    `Required`\n",
    "    :param G1: the first graph to be compared\n",
    "    :param G2: the second graph to be compared\n",
    "    \"\"\"\n",
    "    timed_out = False\n",
    "\n",
    "        \n",
    "    display(Markdown(\"\"\"---\"\"\"))\n",
    "    display(Markdown(\"\"\"Returns GED (graph edit distance) between graphs G1 and G2. \\n\n",
    "    Graph edit distance is a graph similarity measure analogous to Levenshtein distance for strings. \n",
    "    It is defined as minimum cost of edit path (sequence of node and edge edit operations) \n",
    "    transforming graph G1 to graph isomorphic to G2. (Edge weights are not taken into account)\"\"\"))\n",
    "    \n",
    "    loading_indicator = widgets.HTML(value = \"<i class=\\\"fa fa-spinner fa-spin fa-2x fa-fw\\\"></i><span class=\\\"sr-only\\\">Loading...</span>\")\n",
    "    display(loading_indicator)\n",
    "    \n",
    "    #networkx optimized function for graph edit distance\n",
    "    timeout = time.time() + 3\n",
    "    for v in nx.optimize_graph_edit_distance(G1, G2):\n",
    "        minv = v\n",
    "        if time.time() > timeout:\n",
    "            timed_out = True\n",
    "            break\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(\"\"\"Returns GED (graph edit distance). \\n\n",
    "    Graph edit distance is a graph similarity measure analogous to Levenshtein distance for strings. \n",
    "    It is defined as minimum cost of edit path (sequence of node and edge edit operations) \n",
    "    transforming graph G1 to graph isomorphic to G2. (Edge weights are not taken into account)\"\"\"))\n",
    "    if timed_out ==False:\n",
    "        display(Markdown(\"\"\"<strong>Graph edit distance:</strong> \"\"\" +str(minv)))\n",
    "    else: \n",
    "        display(Markdown(\"\"\" \n",
    "        The algorithm is stopping after 10 seconds to not crash the application. \n",
    "        The intermediate result for the graph edit distance is: \"\"\" +str(minv)))\n",
    "        \n",
    "    return\n",
    "        \n",
    "    \n",
    "def highest_probability_chain(G1,G2):\n",
    "    \"\"\"\n",
    "    Remove all edges but the one with the biggest/highest label for each node, then show union network of these \n",
    "    reduced networks\n",
    "    \n",
    "    `Required`\n",
    "    :param G1: the first graph to be compared\n",
    "    :param G2: the second graph to be compared\n",
    "    \"\"\"\n",
    "    H1 = G1.copy()\n",
    "    H2 = G2.copy()\n",
    "    \n",
    "    #remove the edge with the lowest label until only one edge is left\n",
    "    for u in H1.nodes():\n",
    "        while(len(H1.edges(u)) > 1):\n",
    "            min_weight_edge = min(H1.edges(u), key=lambda x: H1.get_edge_data(x[0], x[1])[\"label\"])\n",
    "            H1.remove_edge(*min_weight_edge)\n",
    "\n",
    "    for u in H2.nodes():\n",
    "        while(len(H2.edges(u)) > 1):\n",
    "            min_weight_edge = min(H2.edges(u), key=lambda x: H2.get_edge_data(x[0], x[1])[\"label\"])\n",
    "            H2.remove_edge(*min_weight_edge)\n",
    "    \n",
    "    union_graph(H1,H2)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def show_similarity(algorithm, G1, G2):\n",
    "    \"\"\"\n",
    "    Display output of chosen algorithm and explain it to the user.\n",
    "    \n",
    "    `Required`\n",
    "    :param algorithm: the similarity scoring algorithm to apply\n",
    "    :param G1: the first graph to be compared\n",
    "    :param G2: the second graph to be compared\n",
    "    \"\"\"\n",
    "    if algorithm == 'eigenvector similarity':\n",
    "        eigenvector_similarity(G1,G2)\n",
    "    elif algorithm == 'visualization':\n",
    "        display(Markdown(\"\"\" --- \"\"\"))\n",
    "        display(Markdown(\"\"\"The graphs are combined to identify nodes and edges common to both\"\"\"))\n",
    "        union_graph(G1,G2)\n",
    "    elif algorithm == 'jaccard distance':\n",
    "        jaccard(G1,G2)\n",
    "    elif algorithm == 'graph edit distance':\n",
    "        graph_edit_distance(G1,G2)\n",
    "    elif algorithm == 'highest_probability_chain':\n",
    "        display(Markdown(\"\"\"---\"\"\"))\n",
    "        display(Markdown(\"\"\"For each node, all edges but the one with the highest weight have been removed.\"\"\"))\n",
    "        highest_probability_chain(G1,G2)\n",
    "        \n",
    "    return\n",
    "\n",
    "def compare_networks(G1, G2):\n",
    "    \"\"\"\n",
    "    Calculate network metrics, compare graphs and display output\n",
    "    \n",
    "    `Required` \n",
    "    :param G1: the first graph to be compared\n",
    "    :param G2: the second graph to be compared\n",
    "    \"\"\"\n",
    "    \n",
    "    display(Markdown(\"\"\"---\"\"\"))\n",
    "    \n",
    "    \n",
    "    networks = {\n",
    "        G1.graph['name']: [nx.number_of_nodes(G1),\n",
    "                           nx.number_of_edges(G1),\n",
    "                           nx.number_weakly_connected_components(G1),\n",
    "                           nx.number_strongly_connected_components(G1),\n",
    "                           nx.density(G1), \n",
    "                           nx.transitivity(G1),\n",
    "                           #nx.average_shortest_path_length(G1),\n",
    "                           #avg_degree_centrality(G1)\n",
    "                          ],\n",
    "        G2.graph['name']: [nx.number_of_nodes(G2),\n",
    "                           nx.number_of_edges(G2),\n",
    "                           nx.number_weakly_connected_components(G2),\n",
    "                           nx.number_strongly_connected_components(G2),\n",
    "                           nx.density(G2), \n",
    "                           nx.transitivity(G2),\n",
    "                           #nx.average_shortest_path_length(G2),\n",
    "                           #avg_degree_centrality(G2)\n",
    "                          ]\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame(networks, \n",
    "                      columns = [G1.graph['name'],\n",
    "                                 G2.graph['name']\n",
    "                                ],\n",
    "                      index = ['Number of nodes',\n",
    "                               'Number of edges',\n",
    "                               'Number weakly connected components',\n",
    "                               'Number strongly connected components',\n",
    "                               'Density', \n",
    "                               'Transitivity',\n",
    "                               #'Avg shortest path length',\n",
    "                               #'Avg degree centrality']\n",
    "                              ])\n",
    "    \n",
    "    #change values from float to int where float makes no use\n",
    "    df = df.transpose()\n",
    "    df['Number of nodes'] = df['Number of nodes'].astype('int64')\n",
    "    df['Number of edges'] = df['Number of edges'].astype('int64')\n",
    "    df['Number weakly connected components'] = df['Number weakly connected components'].astype('int64')\n",
    "    df['Number strongly connected components'] = df['Number strongly connected components'].astype('int64')\n",
    "    df = df.astype(object).T\n",
    "    \n",
    "    display(HTML(df.to_html()))\n",
    "    display(Markdown(\"\"\"---\"\"\"))\n",
    "    \n",
    "    \n",
    "    display(Markdown(\"\"\"#### Graph similarity:\"\"\"))\n",
    "    similarity = interactive(show_similarity, algorithm = ['visualization','highest_probability_chain', 'jaccard distance', 'graph edit distance', 'eigenvector similarity' ], G1 = fixed(G1), G2 = fixed(G2)) \n",
    "    display(similarity)\n",
    "    \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare_networks(G1,G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " \n",
       "## Got a corresponding trajectory file? \n",
       "<em>Optionally inspect trajectories/coordinates</em> \n",
       "\n",
       "- <strong>Usage</strong>: Upload file containing the data \n",
       " by clicking <em>Trajectories</em>\n",
       "\n",
       "- <strong>Required columns</strong>: <em>Id</em>, <em>Frame</em>, <em>x</em>, <em>y</em>\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59f7df32de64f7e9e8270e89fd73122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, description='Trajectories', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80aa3e874ad4176affc5b30fe135986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# uploader for the trajectories and corresponding functions\n",
    "\n",
    "def create_distance_network(trajectory_df, max_dist=100, min_seconds=3):\n",
    "    \"\"\"This function takes as parameters the optional coordinates file, the maximal distance for two fish \n",
    "    in a frame for a edge to be drawn between them, and assuming 25 fps, the minimum amount of seconds which\n",
    "    the two fish have to be in the specified distance. \n",
    "    A loop is checking for each frame the distance from each fish to each fish, that is the reason why the output\n",
    "    may take a few seconds until loaded.\"\"\"\n",
    "    #get the range of the frames (i.e. 25/second) to loop through\n",
    "    first_frame = trajectory_df.frame.min()\n",
    "    last_frame = trajectory_df.frame.max()\n",
    "    frames_list = list(range(first_frame, last_frame,5))\n",
    "    close_fish_list = []\n",
    "    #take a slice of trajectory_df for each frame and calculate the \n",
    "    #distances from each fish to each other fish\n",
    "    for frame in frames_list:\n",
    "        frame_df = trajectory_df[trajectory_df.frame == frame]\n",
    "        i=0\n",
    "        while i < len(frame_df)-1:\n",
    "            k=i+1\n",
    "            while k < len(frame_df):\n",
    "                #calculate the distance for fish k and fish i\n",
    "                x = abs(frame_df.x.iloc[i] - frame_df.x.iloc[k])\n",
    "                y = abs(frame_df.y.iloc[i] - frame_df.y.iloc[k])\n",
    "                dist = math.sqrt(x**2 + y**2)\n",
    "                #add an entry to the close_fish_list if dist < threshold\n",
    "                if dist <= max_dist:\n",
    "                    close_fish_list.append((frame_df.id.iloc[i], frame_df.id.iloc[k]))\n",
    "                k+=1\n",
    "            i+=1\n",
    "    #create edges and attributes for network generation\n",
    "    edges_df = pd.DataFrame(close_fish_list, columns=['fish_1', 'fish_2'])\n",
    "    edges_df = edges_df.groupby(edges_df.columns.tolist(), as_index=False).size().to_frame(name='frames').reset_index()\n",
    "    edges_df['tuples'] = list(zip(edges_df.fish_1, edges_df.fish_2))\n",
    "    edges_df['close_seconds'] = edges_df.apply(lambda row: row.frames / 5, axis=1)\n",
    "    edges_df = edges_df[edges_df.close_seconds >= min_seconds]\n",
    "    edge_attributes_label = dict(zip(edges_df.tuples, edges_df.close_seconds))\n",
    "    #change for edge weight\n",
    "    edges_df.close_seconds = edges_df.close_seconds * 3 / edges_df.close_seconds.max()\n",
    "    edge_attributes_weight = dict(zip(edges_df.tuples, edges_df.close_seconds))\n",
    "    #create undirected graph with attributes\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges_df.tuples)\n",
    "    nx.set_edge_attributes(G, edge_attributes_label, name='label')\n",
    "    nx.set_edge_attributes(G, edge_attributes_weight, name='penwidth')\n",
    "    #graphviz\n",
    "    G_dot_string = to_pydot(G).to_string()\n",
    "    G_dot = graphviz.Source(G_dot_string)\n",
    "    G_dot.format= 'svg'\n",
    "    G_dot.render('images/distances.gv', view=False)  \n",
    "    display(HTML('images/distances.gv.svg'))\n",
    "    return \n",
    "\n",
    "def create_trajectory_map(trajectory_df):\n",
    "    \"\"\"Input parameters are the behavior and the coordinates file, the behavior file is used for the IDs, \n",
    "    so the colour scheme of the trajectories taken from the coordinates file is consistent with the colors\n",
    "    from the plots. The trajectories are done by scattering the x- and y- coordinates for each fish for each\n",
    "    frame together in one plot.\"\"\"\n",
    "    global df\n",
    "    #id works only for jakobs positions\n",
    "    fish_ids = trajectory_df.id.unique()\n",
    "    #fish_ids = trajectory_df.id.unique().tolist()\n",
    "    trajectory_list = []\n",
    "    fig = plt.figure(figsize=(9,7))\n",
    "    ax = fig.subplots()\n",
    "    for fish in fish_ids:\n",
    "        #extract positions for the fish and scatter it\n",
    "        coordinates = trajectory_df[trajectory_df.id==fish]\n",
    "        trajectory = ax.scatter(coordinates.x, coordinates.y, 0.1)\n",
    "        trajectory_list.append(trajectory)\n",
    "    plt.legend(trajectory_list, fish_ids, markerscale=20)   \n",
    "    plt.xlabel(\"x-coordinate\", fontsize=18, labelpad=10)\n",
    "    plt.ylabel(\"y-coordinate\", fontsize=18, labelpad=10)\n",
    "    fig.savefig('images/trajectory_map.png', bbox_inches='tight')\n",
    "    return plt\n",
    "\n",
    "def _clean_trajectories(df):\n",
    "    \"\"\"\n",
    "    Delete unneeded header information and standardize column names. \n",
    "    \n",
    "    `Required` \n",
    "    :param df: Panda dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    #If header is not first row, delete rows until one of ['Time', 'time', 'Subject', 'Fps', 'fps', 'subject'] appears\n",
    "    try:\n",
    "        header_row_index = get_row_index(df, ['Id', 'id', 'Frame', 'frame', 'X', 'x'])[0]\n",
    "        df = df.iloc[header_row_index:]\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #all header in lowercase, no spaces\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    df.columns = df.columns.str.replace(' ','_')\n",
    "    \n",
    "    #test if needed column headers are present\n",
    "    headers = ['id', 'frame', 'x', 'y']\n",
    "    for header in headers:\n",
    "        if header not in trajectory_df:\n",
    "            raise Exception(\"Error: column header \" + header + \" is not present.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# a button to upload a file containing behavior data\n",
    "display(Markdown(\"\"\"---\"\"\"))\n",
    "display(Markdown(\n",
    "\"\"\" \n",
    "## Got a corresponding trajectory file? \n",
    "<em>Optionally inspect trajectories/coordinates</em> \\n\n",
    "- <strong>Usage</strong>: Upload file containing the data \\n by clicking <em>Trajectories</em>\\n\n",
    "- <strong>Required columns</strong>: <em>Id</em>, <em>Frame</em>, <em>x</em>, <em>y</em>\\n\n",
    "---\n",
    "\"\"\"))\n",
    "\n",
    "uploader_traj = widgets.FileUpload(description='Trajectories', multiple=True)\n",
    "display(uploader_traj)\n",
    "out_traj = widgets.Output()\n",
    "display(out_traj)\n",
    "\n",
    "def traj_upload_handler(_):\n",
    "    \"\"\"\n",
    "    Handle File Upload. On success, display trajectorie plot and distance network. \n",
    "    On failure, give instructions on what is missing or wrong.\n",
    "    \n",
    "    `Required`\n",
    "    :param change: Indicates new file upload\n",
    "    \"\"\"\n",
    "    \n",
    "    global trajectory_df\n",
    "    upload_sanitized = False\n",
    "    \n",
    "    #read uploaded file into dataframe, display message if wrong file format\n",
    "    [trajectories] = uploader_traj.value\n",
    "    try:\n",
    "        trajectory_df = pd.read_csv(io.BytesIO(uploader_traj.value[trajectories][\"content\"]))\n",
    "    except:\n",
    "        try:\n",
    "            trajectory_df = pd.read_excel(io.BytesIO(uploader_traj.value[trajectories][\"content\"]))\n",
    "        except: \n",
    "            with out_traj:\n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(\"\"\"File must be of type <em>.csv</em> or <em>.xlsx</em>\"\"\"))\n",
    "                return\n",
    "        \n",
    "    #clean file and display message if required header(s) are missing\n",
    "    try:\n",
    "        trajectory_df = _clean_trajectories(trajectory_df)\n",
    "        upload_sanitized = True\n",
    "    except: \n",
    "        with out_traj:\n",
    "            clear_output(wait=True)\n",
    "            if 'id' not in trajectory_df.columns:\n",
    "                display(Markdown(\"\"\" Missing column header: <em>Id/id</em> \"\"\"))\n",
    "            if 'frame' not in trajectory_df.columns:\n",
    "                display(Markdown(\"\"\" Missing column header: <em>Frame/frame</em> \"\"\"))\n",
    "            if 'x' not in trajectory_df.columns:\n",
    "                display(Markdown(\"\"\" Missing column header: <em>X/x</em> \"\"\"))\n",
    "            if 'y' not in trajectory_df.columns:\n",
    "                display(Markdown(\"\"\" Missing column header: <em>Y/y</em> \"\"\"))\n",
    "            \n",
    "    #if data upload and sanitation successful, display information about dataset\n",
    "    if(upload_sanitized):\n",
    "        with out_traj:\n",
    "            clear_output(wait=True)\n",
    "            display(Markdown(\"\"\"---\"\"\"))\n",
    "            display(Markdown(\"\"\"## Trajectory map\"\"\"))\n",
    "            trajectory_map = interactive(create_trajectory_map, trajectory_df = fixed(trajectory_df))\n",
    "            display(trajectory_map)\n",
    "            display(Markdown(\"\"\"## Distance network \n",
    "            \\n The edge label is corresponding to the time in which two individuals are closer to each other than <em>max_dist</em>. \n",
    "            \\n The edge is displayed if the count is bigger than <em>min_seconds</em>. \n",
    "            \\n Be patient: The computation may take a few seconds. \"\"\"))\n",
    "            distance_network = interactive(create_distance_network, trajectory_df = fixed(trajectory_df), max_dist = (10,500,5), min_seconds = (1,600,5))\n",
    "            display(distance_network)\n",
    "            display(Markdown(\"\"\" ---\"\"\"))    \n",
    "            \n",
    "    return\n",
    "\n",
    "#connect on_upload_change function to file upload widget by using its internal counter\n",
    "uploader_traj.observe(traj_upload_handler, names='_counter')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " ---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  <sub><sup>This tool was developed at the University of Constance under supervision of Michael Aichem and Dr. Karsten Klein from the laboratory for Computational Life Sciences. Valuable feedback and data was provided by Etienne Lein, Manh Huy Nguyen, Jakob Guebel and Dr. Alex Jordan from the laboratory for the Evolution of Collective and Social Behavior. The tool is written in Python, using 'networkx' for network generation, 'GraphViz' for drawing and 'voila' in combination with 'heroku' for deploying.  Please send bugs or recommendations to nicolai.kraus@uni-konstanz.de</sup></sub>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\" ---\"\"\"))\n",
    "display(Markdown(\"\"\"  <sub><sup>This tool was developed at the University of Constance under supervision of Michael Aichem and Dr. Karsten Klein from the laboratory for Computational Life Sciences. Valuable feedback and data was provided by Etienne Lein, Manh Huy Nguyen, Jakob Guebel and Dr. Alex Jordan from the laboratory for the Evolution of Collective and Social Behavior. The tool is written in Python, using 'networkx' for network generation, 'GraphViz' for drawing and 'voila' in combination with 'heroku' for deploying.  Please send bugs or recommendations to nicolai.kraus@uni-konstanz.de</sup></sub>\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
