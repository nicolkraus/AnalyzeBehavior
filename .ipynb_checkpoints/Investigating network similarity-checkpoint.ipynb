{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating network similarity for the analysis of fish behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import netrd, networkx as nx\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "lists directory, each element with its full path\n",
    "\"\"\"\n",
    "def listdir_fullpath(d):\n",
    "    return [os.path.join(d, f) for f in os.listdir(d)]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns Jaccard distance for weighted directed networks \n",
    "\"\"\"\n",
    "def weighted_jaccard(g1,g2):\n",
    "    e1 = set(g1.edges)\n",
    "    e2 = set(g2.edges)\n",
    "    cup = set.union(e1, e2)\n",
    "    cap = set.intersection(e1, e2)\n",
    "\n",
    "    #if edge weights are different only add the relative accordance\n",
    "    relative_sum = 0\n",
    "    for edge in cap:\n",
    "        w1 = g1.get_edge_data(edge[0], edge[1])[\"label\"]\n",
    "        w2 = g2.get_edge_data(edge[0], edge[1])[\"label\"]\n",
    "        if w1 > w2:\n",
    "            relative_sum = relative_sum + (w2 / w1)\n",
    "        elif w2 > w1:\n",
    "            relative_sum = relative_sum + (w1 / w2)\n",
    "        else:\n",
    "            relative_sum = relative_sum +1\n",
    "\n",
    "    dist = 1 - relative_sum / len(cup)\n",
    "    return dist\n",
    "\n",
    "\n",
    "#\"\"\"\n",
    "#Returns Frobenius norm for weighted directed graphs.\n",
    "#\"\"\"\n",
    "#def weighted_frobenius(g1,g2):\n",
    "#\n",
    "#    #print(nx.get_edge_attributes(g1, label))\n",
    "#    not_in_g1 = np.setdiff1d(g2.nodes(),g1.nodes())\n",
    " #   g1.add_nodes_from(not_in_g1)\n",
    " #   not_in_g2 = np.setdiff1d(g1.nodes(),g2.nodes())\n",
    "  #  g2.add_nodes_from(not_in_g2)\n",
    "   # \n",
    "   # adj1 = nx.to_numpy_array(g1, weight=\"label\")\n",
    "   # adj2 = nx.to_numpy_array(g2, weight=\"label\")\n",
    "   # dist = np.linalg.norm((adj1 - adj2))\n",
    "   # \n",
    "   # return dist\n",
    "\n",
    "\n",
    "def edge_subst(g1edge, g2edge):\n",
    "    cost = abs(g1edge['label']-g2edge['label'])\n",
    "    return cost\n",
    "    \n",
    "def edge_cost(edge):\n",
    "    cost = abs(edge['label'])\n",
    "    return cost\n",
    "\n",
    "def node_cost(node):\n",
    "    print(node)\n",
    "\n",
    "\"\"\"\n",
    "Returns GED \n",
    "\"\"\"\n",
    "def weighted_frobenius(g1,g2):\n",
    "    dist = nx.graph_edit_distance(g1, g2, edge_subst_cost=edge_subst, edge_ins_cost=edge_cost, edge_del_cost=edge_cost, timeout=10)\n",
    "    print(\"done\")\n",
    "    return dist\n",
    "\n",
    "#lists with the files\n",
    "oce_bvr = listdir_fullpath(\"/Users/nicol/Desktop/bachelorthesis/data/normalized/ocellatus-behaviours/\")\n",
    "mul_bvr = listdir_fullpath(\"/Users/nicol/Desktop/bachelorthesis/data/normalized/multi-behaviours/\")\n",
    "tem_bvr = listdir_fullpath(\"/Users/nicol/Desktop/bachelorthesis/data/normalized/tempo-behaviours/\")\n",
    "\n",
    "oce_ctg = listdir_fullpath(\"/Users/nicol/Desktop/bachelorthesis/data/normalized/ocellatus-categories/\")\n",
    "mul_ctg = listdir_fullpath(\"/Users/nicol/Desktop/bachelorthesis/data/normalized/multi-categories/\")\n",
    "tem_ctg = listdir_fullpath(\"/Users/nicol/Desktop/bachelorthesis/data/normalized/tempo-categories/\")\n",
    "\n",
    "behaviours = [oce_bvr, mul_bvr, tem_bvr]\n",
    "categories = [oce_ctg, mul_ctg, tem_ctg]\n",
    "\n",
    "# list with algorithms to apply\n",
    "distance_algs =  [netrd.distance.Hamming(),\n",
    "                  netrd.distance.Frobenius(),\n",
    "                  'weighted_frobenius',\n",
    "                  netrd.distance.PortraitDivergence(),\n",
    "                  netrd.distance.JaccardDistance(), \n",
    "                  'weighted_jaccard',\n",
    "]\n",
    "#distance_algs = [netrd.distance.DeltaCon()]\n",
    "#names \n",
    "algs = ['hamming', 'frobenius', 'weighted_frobenius', 'portrait_divergence', \n",
    "        'jaccard_distance', 'weighted_jaccard', 'ged']\n",
    "#algs = ['deltacon']\n",
    "#istance_algs = [netrd.distance.PortraitDivergence()]\n",
    "same_nodes_algs = ['hamming', 'frobenius', 'deltacon']\n",
    "\n",
    "results= []\n",
    "df = pd.DataFrame(columns=['alg', 'data1', 'data2', 'results'])\n",
    "general_df = pd.DataFrame(columns=['alg', 'data', 'type', 'results'])\n",
    "\n",
    "for alg in distance_algs:\n",
    "    if alg == 'weighted_jaccard':\n",
    "        alg_name = alg\n",
    "    elif alg == 'weighted_frobenius':\n",
    "        alg_name = alg\n",
    "    else:\n",
    "        alg_name = str(alg).split(\".\")[2]\n",
    "    print(alg_name)\n",
    "    \n",
    "    #lists for specieal results\n",
    "    bhvr_across_species = []\n",
    "    bhvr_within_species = []\n",
    "    cat_across_species = []\n",
    "    cat_within_species = []\n",
    "    \n",
    "    #BEHAVIOURS PAIRWISE\n",
    "    for data1, data2 in itertools.combinations(behaviours, 2):\n",
    "        #folder names for adding in dataframe\n",
    "        folder1 = str(data1).split(\"/\")[7]\n",
    "        folder2 = str(data2).split(\"/\")[7]\n",
    "        #compare each folder with each other\n",
    "        for a in data1:\n",
    "            for b in data2:\n",
    "\n",
    "                #read gpickles into networkx\n",
    "                g1 = nx.read_gpickle(a)\n",
    "                g2 = nx.read_gpickle(b)\n",
    "                \n",
    "                for edge in g1.edges:\n",
    "                    weight = g1.get_edge_data(edge[0],edge[1])['label']\n",
    "                    g1[edge[0]][edge[1]]['weight'] = weight\n",
    "                    \n",
    "                for edge in g2.edges:\n",
    "                    weight = g2.get_edge_data(edge[0],edge[1])['label']\n",
    "                    g2[edge[0]][edge[1]]['weight'] = weight\n",
    "\n",
    "                # if alg requires same nodeset, add nodes\n",
    "                if alg_name in same_nodes_algs:    \n",
    "                    not_in_g1 = np.setdiff1d(g2.nodes(),g1.nodes())\n",
    "                    g1.add_nodes_from(not_in_g1)\n",
    "                    not_in_g2 = np.setdiff1d(g1.nodes(),g2.nodes())\n",
    "                    g2.add_nodes_from(not_in_g2)\n",
    "\n",
    "                #calculate and append results\n",
    "                if alg_name == 'weighted_jaccard':\n",
    "                    results.append(weighted_jaccard(g1,g2))\n",
    "                elif alg_name == 'weighted_frobenius':\n",
    "                    results.append(weighted_frobenius(g1,g2))\n",
    "                else:\n",
    "                    dist = alg\n",
    "                    d = dist.dist(g1, g2)\n",
    "                    results.append(d)\n",
    "               \n",
    "\n",
    "\n",
    "        #calculate statistics and append to across species list\n",
    "        df.loc[len(df.index)] = [alg_name,folder1, folder2, results]\n",
    "        bhvr_across_species.extend(results)\n",
    "        results = []\n",
    "        \n",
    "    #add row for behaviours across species\n",
    "    general_df.loc[len(general_df.index)] = [alg_name, 'across species', 'behaviours', bhvr_across_species]    \n",
    "        \n",
    "        \n",
    "    #CATEGORIES PAIRWISE\n",
    "    for data1, data2 in itertools.combinations(categories, 2):\n",
    "        #folder names for adding in dataframe\n",
    "        folder1 = str(data1).split(\"/\")[7]\n",
    "        folder2 = str(data2).split(\"/\")[7]\n",
    "        #compare each folder with each other\n",
    "        for a in data1:\n",
    "            for b in data2:\n",
    "\n",
    "                #read gpickles into networkx\n",
    "                g1 = nx.read_gpickle(a)\n",
    "                g2 = nx.read_gpickle(b)\n",
    "\n",
    "                # if alg requires same nodeset, add nodes\n",
    "                if alg_name in same_nodes_algs:    \n",
    "                    not_in_g1 = np.setdiff1d(g2.nodes(),g1.nodes())\n",
    "                    g1.add_nodes_from(not_in_g1)\n",
    "                    not_in_g2 = np.setdiff1d(g1.nodes(),g2.nodes())\n",
    "                    g2.add_nodes_from(not_in_g2)\n",
    "\n",
    "               \n",
    "                #calculate and append results\n",
    "                if alg_name == 'weighted_jaccard':\n",
    "                    results.append(weighted_jaccard(g1,g2))\n",
    "                elif alg_name == 'weighted_frobenius':\n",
    "                    results.append(weighted_frobenius(g1,g2))\n",
    "                else:\n",
    "                    dist = alg\n",
    "                    d = dist.dist(g1, g2)\n",
    "                    results.append(d)\n",
    "                \n",
    "        #calculate statistics\n",
    "        df.loc[len(df.index)] = [alg_name,folder1, folder2, results]\n",
    "        cat_across_species.extend(results)\n",
    "        results = []\n",
    "        \n",
    "    #add row for behaviours across species\n",
    "    general_df.loc[len(general_df.index)] = [alg_name, 'across species', 'behavioural category', cat_across_species]  \n",
    "        \n",
    "    #CATEGORIES SINGLE\n",
    "    for data in categories:\n",
    "        #folder names for adding in dataframe\n",
    "        folder1 = str(data).split(\"/\")[7]\n",
    "        folder2 = str(data).split(\"/\")[7]\n",
    "        #compare each folder with each other\n",
    "        for a,b in itertools.combinations(data, 2):\n",
    "\n",
    "                #read gpickles into networkx\n",
    "                g1 = nx.read_gpickle(a)\n",
    "                g2 = nx.read_gpickle(b)\n",
    "\n",
    "                # if alg requires same nodeset, add nodes\n",
    "                if alg_name in same_nodes_algs:    \n",
    "                    not_in_g1 = np.setdiff1d(g2.nodes(),g1.nodes())\n",
    "                    g1.add_nodes_from(not_in_g1)\n",
    "                    not_in_g2 = np.setdiff1d(g1.nodes(),g2.nodes())\n",
    "                    g2.add_nodes_from(not_in_g2)\n",
    "\n",
    "                \n",
    "                #calculate and append results\n",
    "                if alg_name == 'weighted_jaccard':\n",
    "                    results.append(weighted_jaccard(g1,g2))\n",
    "                elif alg_name == 'weighted_frobenius':\n",
    "                    results.append(weighted_frobenius(g1,g2))\n",
    "                else:\n",
    "                    dist = alg\n",
    "                    d = dist.dist(g1, g2)\n",
    "                    results.append(d)\n",
    "\n",
    "        #calculate statistics\n",
    "        df.loc[len(df.index)] = [alg_name,folder1, folder2, results]\n",
    "        cat_within_species.extend(results)\n",
    "        results = []\n",
    "        \n",
    "    #add row for behaviours across species\n",
    "    general_df.loc[len(general_df.index)] = [alg_name, 'within species', 'behavioural category', cat_within_species]  \n",
    "        \n",
    "        \n",
    "    #BEHAVIOURS SINGLE\n",
    "    for data in behaviours:\n",
    "        #folder names for adding in dataframe\n",
    "        folder1 = str(data).split(\"/\")[7]\n",
    "        folder2 = str(data).split(\"/\")[7]\n",
    "        #compare each folder with each other\n",
    "        for a,b in itertools.combinations(data, 2):\n",
    "\n",
    "                #read gpickles into networkx\n",
    "                g1 = nx.read_gpickle(a)\n",
    "                g2 = nx.read_gpickle(b)\n",
    "\n",
    "                # if alg requires same nodeset, add nodes\n",
    "                if alg_name in same_nodes_algs:    \n",
    "                    not_in_g1 = np.setdiff1d(g2.nodes(),g1.nodes())\n",
    "                    g1.add_nodes_from(not_in_g1)\n",
    "                    not_in_g2 = np.setdiff1d(g1.nodes(),g2.nodes())\n",
    "                    g2.add_nodes_from(not_in_g2)\n",
    "\n",
    "               \n",
    "                #calculate and append results\n",
    "                if alg_name == 'weighted_jaccard':\n",
    "                    results.append(weighted_jaccard(g1,g2))\n",
    "                elif alg_name == 'weighted_frobenius':\n",
    "                    results.append(weighted_frobenius(g1,g2))\n",
    "                else:\n",
    "                    dist = alg\n",
    "                    d = dist.dist(g1, g2)\n",
    "                    results.append(d)\n",
    "\n",
    "        #calculate statistics\n",
    "        df.loc[len(df.index)] = [alg_name,folder1, folder2, results]\n",
    "        bhvr_within_species.extend(results)\n",
    "        results = []\n",
    "        \n",
    "    #add row for behaviours across species\n",
    "    general_df.loc[len(general_df.index)] = [alg_name, 'within species', 'behaviours', bhvr_within_species] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column 'type' and remove type from data-columns\n",
    "df['type'] = df.data1\n",
    "\n",
    "#changing type \n",
    "type_mapping = [\n",
    "    (df['type'] == 'multi-categories', 'behavioural category'),\n",
    "    (df['type'] == 'tempo-categories', 'behavioural category'),\n",
    "    (df['type'] == 'ocellatus-categories', 'behavioural category'),\n",
    "    \n",
    "    (df['type'] == 'ocellatus-behaviours', 'behaviour'),\n",
    "    (df['type'] == 'multi-behaviours', 'behaviour'),\n",
    "    (df['type'] == 'tempo-behaviours', 'behaviour'),\n",
    "]\n",
    "condlist = [item[0] for item in type_mapping]\n",
    "choicelist = [item[1] for item in type_mapping]\n",
    "\n",
    "\n",
    "df['type'] = np.select(condlist, choicelist)\n",
    "\n",
    "#changing data column\n",
    "data1_mapping = [\n",
    "    (df['data1'] == 'multi-categories', 'multi'),\n",
    "    (df['data1'] == 'tempo-categories', 'tempo'),\n",
    "    (df['data1'] == 'ocellatus-categories', 'oclts'),\n",
    "    \n",
    "    (df['data1'] == 'ocellatus-behaviours', 'oclts'),\n",
    "    (df['data1'] == 'multi-behaviours', 'multi'),\n",
    "    (df['data1'] == 'tempo-behaviours', 'tempo'),\n",
    "]\n",
    "condlist = [item[0] for item in data1_mapping]\n",
    "choicelist = [item[1] for item in data1_mapping]\n",
    "\n",
    "\n",
    "df['data1'] = np.select(condlist, choicelist)\n",
    "\n",
    "#changing data2 column\n",
    "data1_mapping = [\n",
    "    (df['data2'] == 'multi-categories', 'multi'),\n",
    "    (df['data2'] == 'tempo-categories', 'tempo'),\n",
    "    (df['data2'] == 'ocellatus-categories', 'oclts'),\n",
    "    \n",
    "    (df['data2'] == 'ocellatus-behaviours', 'oclts'),\n",
    "    (df['data2'] == 'multi-behaviours', 'multi'),\n",
    "    (df['data2'] == 'tempo-behaviours', 'tempo'),\n",
    "]\n",
    "condlist = [item[0] for item in data1_mapping]\n",
    "choicelist = [item[1] for item in data1_mapping]\n",
    "\n",
    "\n",
    "df['data2'] = np.select(condlist, choicelist)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teste alles manuell auf GED, dann eintragen\n",
    "beh_oclts = [47.0,46.0,50.0,62.0,28.0,37.0,61.0,40.0,25.0,23.0,83.0,87.0,73.0,74.0,70.0,81.0,66.0,72.0,40.0,46.0,35.0,65,46.0,57,53,68,77,48,75,78,77,51,48,75,67,82,79,50,55,81,73,37,53,38,55,51,62,41,58,87,84,57,63,46]\n",
    "beh_multi = [41,75,74,62,70,54,73,85,65,70,61,55,79,46,71,52,48,62,61,77]\n",
    "beh_tempo = [77,63,62,24,15,13]\n",
    "beh_oclts_multi = [50,67,62,64,56,46,85,43,69,47,76,53,63,83,57,76,77,94,52,73,68,47,64,57,73,73,51,62,76,46,42,49,70,91,78,66,80,72,79,89,82,79,50,78,66,62,74,67,67,87,72,67,51,66,81,64,68,60,80,85,61,65,71,63,82,61,67,77,62,74,60,68,61]\n",
    "beh_oclts_tempo = [70,80,85,74,74,80,93,81,67,72,64,19,55,43,63,61,66,50,55,44,34,25,52,57,59,67,43,49,66,44,30,52,16,64,67,64,46,61,73,33,58,45]\n",
    "beh_multi_tempo = [86,49,39,50,81,70,54,59,81,44,37,68,67,58,64,57,38,66,67,54,91,52,57,58]\n",
    "\n",
    "cat_oclts = [10,7,12,17,2,8,12,11,9,2,5,2,7,8,2,2,3,10,5,10,5,5,4,4,7,5,10,4,2,1,3,12,15,9,5,6,8,17,6,10,9,7,2,4,3,3,8,1,3,12,2,11,9]\n",
    "cat_multi = [14,15,5,15,3,11,8,8,8,18,14,6,6,9,11,11,15,19,14,15,8,10]\n",
    "cat_tempo = [17,19,24,6,9,7]\n",
    "cat_oclts_multi = [15,7,8,5,2,13,7,5,2,13,7,5,6,6,15,27,17,20,15,10,25,19,15,16,18,27,18,8,11,6,1,16,10,6,7,9,18,1,9,12,9,8,17,11,9,10,12,1,33,23,26,21,16,31,25,21,22,24,33,29,19,22,17,12,27,21,17,18,20,29]\n",
    "cat_oclts_tempo = [31,21,24,19,14,29,23,19,20,22,31,14,6,7,4,5,12,8,6,5,7,14,12,6,9,4,5,3,12,7,7,10,9,10,7,7,9,8,7]\n",
    "cat_multi_tempo = [16,6,17,13,12,6,6,3,15,6,4,9,19,15,5,15,8,6,7,21,17,8,20,11,11,12,26,22]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually adding rows to df for later presentation\n",
    "df.loc[len(df.index)] = ['ged', 'oclts', 'multi', beh_oclts_multi, 'behaviour'] \n",
    "df.loc[len(df.index)] = ['ged', 'oclts', 'tempo', beh_oclts_tempo, 'behaviour'] \n",
    "df.loc[len(df.index)] = ['ged', 'multi', 'tempo', beh_multi_tempo, 'behaviour'] \n",
    "\n",
    "df.loc[len(df.index)] = ['ged', 'oclts', 'multi', cat_oclts_multi, 'behavioural category'] \n",
    "df.loc[len(df.index)] = ['ged', 'oclts', 'tempo', cat_oclts_tempo, 'behavioural category'] \n",
    "df.loc[len(df.index)] = ['ged', 'multi', 'tempo', cat_multi_tempo, 'behavioural category'] \n",
    "\n",
    "df.loc[len(df.index)] = ['ged', 'oclts', 'oclts', cat_oclts, 'behavioural category'] \n",
    "df.loc[len(df.index)] = ['ged', 'multi', 'multi', cat_multi, 'behavioural category'] \n",
    "df.loc[len(df.index)] = ['ged', 'tempo', 'tempo', cat_tempo, 'behavioural category']\n",
    "\n",
    "df.loc[len(df.index)] = ['ged', 'oclts', 'oclts', beh_oclts, 'behaviour'] \n",
    "df.loc[len(df.index)] = ['ged', 'multi', 'multi', beh_multi, 'behaviour'] \n",
    "df.loc[len(df.index)] = ['ged', 'tempo', 'tempo', beh_tempo, 'behaviour'] \n",
    "\n",
    "#manually adding rows to general_df for later representation\n",
    "beh_across_species = beh_oclts_multi + beh_oclts_tempo + beh_multi_tempo\n",
    "beh_within_species = beh_oclts + beh_multi + beh_tempo\n",
    "cat_across_species = cat_oclts_multi + cat_oclts_tempo + cat_multi_tempo\n",
    "cat_within_species = cat_oclts + cat_tempo + cat_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df.loc[len(general_df.index)] = ['ged', 'across species', 'behaviours', beh_across_species]\n",
    "general_df.loc[len(general_df.index)] = ['ged', 'across species', 'behavioural category', cat_across_species]\n",
    "general_df.loc[len(general_df.index)] = ['ged', 'within species', 'behavioural category', cat_within_species]\n",
    "general_df.loc[len(general_df.index)] = ['ged', 'within species', 'behaviours', beh_within_species]\n",
    "\n",
    "\n",
    "general_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plots\n",
    "erste zeile ist titel, \n",
    "2te (2te und 3te) sind x-achsenabschnitte,\n",
    "median,max,min sind spritzen-y-gemappt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "algs = ['hamming', 'frobenius', 'weighted_frobenius', 'portrait_divergence', \n",
    "        'jaccard_distance', 'weighted_jaccard']\n",
    "\n",
    "for alg in algs:\n",
    "    alg_df = df[df.alg == alg]\n",
    "    \n",
    "    #behavioural category data gen\n",
    "    bhvr_df = alg_df[alg_df.type == 'behavioural category']\n",
    "    data = []\n",
    "    labels = []\n",
    "    i=0\n",
    "    while (i < len(bhvr_df)):\n",
    "        data.append(bhvr_df.iloc[i]['results'])\n",
    "        labels.append(bhvr_df.iloc[i]['data1'] + \" - \" +bhvr_df.iloc[i]['data2'])\n",
    "        i = i+1\n",
    "        \n",
    "    #behaviour data gen\n",
    "    bhvr_df = alg_df[alg_df.type == 'behaviour']\n",
    "    data_bhvr = []\n",
    "    labels_bhvr = []\n",
    "    i=0\n",
    "    while (i < len(bhvr_df)):\n",
    "        data_bhvr.append(bhvr_df.iloc[i]['results'])\n",
    "        labels_bhvr.append(bhvr_df.iloc[i]['data1'] + \" - \" +bhvr_df.iloc[i]['data2'])\n",
    "        i = i+1\n",
    "        \n",
    "       \n",
    "    #behavioural category plot\n",
    "    c = 'red'\n",
    "    fig, axs = plt.subplots(2,2, figsize=(18,8))\n",
    "    axs[0,0].boxplot(data[:3], positions=[1,2,3], notch=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor=c, color=c),\n",
    "            capprops=dict(color=c),\n",
    "            whiskerprops=dict(color=c),\n",
    "            flierprops=dict(color=c, markeredgecolor=c),\n",
    "            medianprops=dict(color=c),\n",
    "            )\n",
    "    c='blue'\n",
    "    axs[0,0].boxplot(data[3:], positions=[4,5,6], notch=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor=c, color=c),\n",
    "            capprops=dict(color=c),\n",
    "            whiskerprops=dict(color=c),\n",
    "            flierprops=dict(color=c, markeredgecolor=c),\n",
    "            medianprops=dict(color=c),\n",
    "            )\n",
    "    axs[0,0].set_title('Networks on Behavioural Categories', size=16)\n",
    "    axs[0,0].set_xticklabels(labels)\n",
    "    \n",
    "    #behaviour plot\n",
    "    c = 'red'\n",
    "    axs[0,1].boxplot(data_bhvr[:3], positions=[1,2,3], notch=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor=c, color=c),\n",
    "            capprops=dict(color=c),\n",
    "            whiskerprops=dict(color=c),\n",
    "            flierprops=dict(color=c, markeredgecolor=c),\n",
    "            medianprops=dict(color=c),\n",
    "            )\n",
    "    c='blue'\n",
    "    axs[0,1].boxplot(data_bhvr[3:], positions=[4,5,6], notch=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor=c, color=c),\n",
    "            capprops=dict(color=c),\n",
    "            whiskerprops=dict(color=c),\n",
    "            flierprops=dict(color=c, markeredgecolor=c),\n",
    "            medianprops=dict(color=c),\n",
    "            \n",
    "            )\n",
    "    axs[0,1].set_title('Networks on Behaviours', size=16)\n",
    "    axs[0,1].set_xticklabels(labels_bhvr)\n",
    "    \n",
    "    #title\n",
    "    if (alg == 'weighted_jaccard'):\n",
    "        fig.suptitle(\"Weighted Jaccard Distances for\", size=18)\n",
    "    elif (alg == 'jaccard_distance'):\n",
    "        fig.suptitle(\"Jaccard Distances for\", size=18)\n",
    "    elif (alg == 'hamming'):\n",
    "        fig.suptitle(\"Hamming Distances for\", size=18)\n",
    "    elif (alg == 'frobenius'):\n",
    "        fig.suptitle(\"Frobenius Distances (excluding edge weights) for\", size=18)\n",
    "    elif (alg == 'portrait_divergence'):\n",
    "        fig.suptitle(\"Network Portrait Divergences for\", size=18)\n",
    "    elif (alg == 'ged'):\n",
    "        fig.suptitle(\"Graph Edit Distances for\", size=18)\n",
    "    elif (alg == 'weighted_frobenius'):\n",
    "        fig.suptitle(\"Frobenius Distances (including edge weights) for\", size=18)\n",
    "    else:\n",
    "        fig.suptitle(alg.upper(), size=16)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# generalized data plots below normal data\n",
    "\n",
    "    alg_df = general_df[general_df.alg == alg]\n",
    "\n",
    "    #behavioural category data gen\n",
    "    bhvr_df = alg_df[alg_df.type == 'behavioural category']\n",
    "    data = []\n",
    "    labels = []\n",
    "    i=0\n",
    "    while (i < len(bhvr_df)):\n",
    "        data.append(bhvr_df.iloc[i]['results'])\n",
    "        labels.append(bhvr_df.iloc[i]['data'])\n",
    "        i = i+1\n",
    "\n",
    "    #behaviour data gen\n",
    "    bhvr_df = alg_df[alg_df.type == 'behaviours']\n",
    "    data_bhvr = []\n",
    "    labels_bhvr = []\n",
    "    i=0\n",
    "    while (i < len(bhvr_df)):\n",
    "        data_bhvr.append(bhvr_df.iloc[i]['results'])\n",
    "        labels_bhvr.append(bhvr_df.iloc[i]['data'])\n",
    "        i = i+1\n",
    "\n",
    "       \n",
    "    #behavioural category plot\n",
    "    c = 'red'\n",
    "    axs[1,0].boxplot(data[:1], positions=[1], notch=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor=c, color=c),\n",
    "            capprops=dict(color=c),\n",
    "            whiskerprops=dict(color=c),\n",
    "            flierprops=dict(color=c, markeredgecolor=c),\n",
    "            medianprops=dict(color=c),\n",
    "            )\n",
    "    c='blue'\n",
    "    axs[1,0].boxplot(data[1:], positions=[2], notch=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor=c, color=c),\n",
    "            capprops=dict(color=c),\n",
    "            whiskerprops=dict(color=c),\n",
    "            flierprops=dict(color=c, markeredgecolor=c),\n",
    "            medianprops=dict(color=c),\n",
    "            )\n",
    "    axs[1,0].set_xticklabels(labels, size=15)\n",
    "    \n",
    "    \n",
    "    #general behaviour plot\n",
    "    c = 'red'\n",
    "    axs[1,1].boxplot(data_bhvr[:1], positions=[1], notch=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor=c, color=c),\n",
    "            capprops=dict(color=c),\n",
    "            whiskerprops=dict(color=c),\n",
    "            flierprops=dict(color=c, markeredgecolor=c),\n",
    "            medianprops=dict(color=c),\n",
    "            )\n",
    "    c='blue'\n",
    "    axs[1,1].boxplot(data_bhvr[1:], positions=[2], notch=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor=c, color=c),\n",
    "            capprops=dict(color=c),\n",
    "            whiskerprops=dict(color=c),\n",
    "            flierprops=dict(color=c, markeredgecolor=c),\n",
    "            medianprops=dict(color=c),\n",
    "            )\n",
    "   \n",
    "    axs[1,1].set_xticklabels(labels_bhvr, size=15)\n",
    "    \n",
    "    \n",
    "    #set yticks\n",
    "    if (alg == 'ged'):\n",
    "        axs[1,1].yaxis.set_ticks(np.arange(0, 100, 10))\n",
    "        axs[0,1].yaxis.set_ticks(np.arange(0, 100, 10))\n",
    "        axs[1,0].yaxis.set_ticks(np.arange(0, 100, 10))\n",
    "        axs[0,0].yaxis.set_ticks(np.arange(0, 100, 10))\n",
    "        \n",
    "    elif (alg == 'frobenius'):\n",
    "        axs[1,1].yaxis.set_ticks(np.arange(0, 12, 1))\n",
    "        axs[0,1].yaxis.set_ticks(np.arange(0, 12, 1))\n",
    "        axs[1,0].yaxis.set_ticks(np.arange(0, 12, 1))\n",
    "        axs[0,0].yaxis.set_ticks(np.arange(0, 12, 1))\n",
    "        \n",
    "    elif (alg == 'weighted_frobenius'):\n",
    "        axs[1,1].yaxis.set_ticks(np.arange(0, 6, 0.5))\n",
    "        axs[0,1].yaxis.set_ticks(np.arange(0, 6, 0.5))\n",
    "        axs[1,0].yaxis.set_ticks(np.arange(0, 6, 0.5))\n",
    "        axs[0,0].yaxis.set_ticks(np.arange(0, 6, 0.5))\n",
    "        \n",
    "    else:\n",
    "        axs[1,1].yaxis.set_ticks(np.arange(0, 1, 0.1))\n",
    "        axs[0,1].yaxis.set_ticks(np.arange(0, 1, 0.1))\n",
    "        axs[1,0].yaxis.set_ticks(np.arange(0, 1, 0.1))\n",
    "        axs[0,0].yaxis.set_ticks(np.arange(0, 1, 0.1))\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Adding title and legend\n",
    "    #red_patch = mpatches.Patch(color='red', label='Comparison ACROSS species')\n",
    "    #blue_patch = mpatches.Patch(color='blue', label='Comparison WITHIN species')\n",
    "    #fig.legend(handles=[red_patch, blue_patch])\n",
    "    # show plot\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weighted jaccard\n",
    "\n",
    "- edge label auf edge mappen\n",
    "- kleineren durch größeren teilen, dann schnittmenge dementsprechend verkleinern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns Jaccard distance for weighted directed networks \n",
    "\"\"\"\n",
    "def weighted_jaccard(g1,g2):\n",
    "    e1 = set(g1.edges)\n",
    "    e2 = set(g2.edges)\n",
    "    cup = set.union(e1, e2)\n",
    "    cap = set.intersection(e1, e2)\n",
    "\n",
    "    #if edge weights are different only add the relative accordance\n",
    "    relative_sum = 0\n",
    "    for edge in cap:\n",
    "\n",
    "        w1 = g1.get_edge_data(edge[0], edge[1])[\"label\"]\n",
    "        w2 = g2.get_edge_data(edge[0], edge[1])[\"label\"]\n",
    "        if w1 > w2:\n",
    "            relative_sum = relative_sum + (w2 / w1)\n",
    "        elif w2 > w1:\n",
    "            relative_sum = relative_sum + (w1 / w2)\n",
    "        else:\n",
    "            relative_sum = relative_sum +1\n",
    "\n",
    "    dist = 1 - relative_sum / len(cup)\n",
    "    return dist\n",
    "    \n",
    "g1 = nx.read_gpickle(\"/Users/nicol/Desktop/bachelorthesis/data/multi-categories/Multi_12_10_17_a2_first_15_02_19.gpickle\")\n",
    "g2 = nx.read_gpickle(\"/Users/nicol/Desktop/bachelorthesis/data/multi-categories/Multi_12_10_17_e1_first_19_02_19.gpickle\")\n",
    "\n",
    "dist = weighted_jaccard(g1,g2)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#GED not working, laptop is failing\n",
    "\n",
    "results= []\n",
    "networkx_algs = ['ged']\n",
    "for alg in networkx_algs:\n",
    "    alg_name = alg\n",
    "    \n",
    "    #BEHAVIOURS PAIRWISE\n",
    "    for data1, data2 in itertools.combinations(behaviours, 2):\n",
    "        #folder names for adding in dataframe\n",
    "        folder1 = str(data1).split(\"/\")[6]\n",
    "        folder2 = str(data2).split(\"/\")[6]\n",
    "        #compare each folder with each other\n",
    "        for a in data1:\n",
    "            for b in data2:\n",
    "\n",
    "                #read gpickles into networkx\n",
    "                g1 = nx.read_gpickle(a)\n",
    "                g2 = nx.read_gpickle(b)\n",
    "\n",
    "                #networkx optimized function for graph edit distance\n",
    "                timeout = time.time() + 5\n",
    "                for v in nx.optimize_graph_edit_distance(g1, g2):\n",
    "                    minv = v\n",
    "                    print(time.time())\n",
    "                    if time.time() > timeout:\n",
    "                        break\n",
    "                results.append(minv)\n",
    "                print(minv)\n",
    "\n",
    "\n",
    "        #calculate statistics\n",
    "        df.loc[len(df.index)] = [alg_name,folder1, folder2, results]\n",
    "        results = []\n",
    "        \n",
    "        \n",
    "df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pie chart for threads and limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make the pie circular by setting the aspect ratio to 1\n",
    "plt.figure(figsize=(10, 5))\n",
    "nur_o = 5\n",
    "om = 1\n",
    "mt = 2\n",
    "rest = 17\n",
    "values = [nur_o, om, mt, rest]\n",
    "labels = [\"L. Ocellatus\",\"L. Ocellatus & N. Multifasciatus\", \"T. Temporalis & N. Multifasciatus\", \"All species\" ]\n",
    "\n",
    "def make_autopct(values):\n",
    "    def my_autopct(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct*total/100.0))\n",
    "        return '{p:.2f}%  ({v:d})'.format(p=pct,v=val)\n",
    "    return my_autopct\n",
    "\n",
    "plt.pie(values, labels=labels, autopct=make_autopct(values), startangle=90, textprops = dict(size=11.2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig, ax = plt.subplots(figsize=(10, 5), subplot_kw=dict(aspect=\"equal\"))\n",
    "\n",
    "recipe = [\"5 L. Ocellatus\",\n",
    "          \"1 L. Ocellatus & N. Multifasciatus\",\n",
    "          \"2 N. Multifasciatus & T. Temporalis\",\n",
    "          \"17 All species\"]\n",
    "\n",
    "data = [float(x.split()[0]) for x in recipe]\n",
    "ingredients = [x.split()[1:] for x in recipe]\n",
    "ingredients[2] = ' '.join(ingredients[2])\n",
    "ingredients[3] = ' '.join(ingredients[3])\n",
    "ingredients[1] = ' '.join(ingredients[1])\n",
    "ingredients[0] = ' '.join(ingredients[0])\n",
    " \n",
    "def func(pct, allvals):\n",
    "    absolute = int(round(pct/100.*np.sum(allvals)))\n",
    "    return \"{:.1f}%\\n({:d} )\".format(pct, absolute)\n",
    "\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func(pct, data),\n",
    "                                  textprops=dict(color=\"w\"), startangle = 90, shadow=True)\n",
    "\n",
    "ax.legend(wedges, ingredients,\n",
    "          title=\"Species\",\n",
    "          loc=\"center left\",\n",
    "          bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "plt.setp(autotexts, size=10, weight=\"bold\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import netrd, networkx as nx\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "lists directory, each element with its full path\n",
    "\"\"\"\n",
    "def listdir_fullpath(d):\n",
    "    return [os.path.join(d, f) for f in os.listdir(d)]\n",
    "\n",
    "\n",
    "oce = listdir_fullpath(\"/Users/nicol/Desktop/bachelorthesis/data/0o/\")\n",
    "tem = listdir_fullpath(\"/Users/nicol/Desktop/bachelorthesis/data/0t/\")\n",
    "mul = listdir_fullpath(\"/Users/nicol/Desktop/bachelorthesis/data/0m/\")\n",
    "\n",
    "oce_sizes = []\n",
    "tem_sizes = []\n",
    "mul_sizes = []\n",
    "\n",
    "for elem in oce:\n",
    "    df = pd.read_excel(elem)\n",
    "    df = df.iloc[14:]\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.iloc[1:]\n",
    "    oce_sizes.append(len(df))\n",
    "    \n",
    "for elem in tem:\n",
    "    df = pd.read_excel(elem)\n",
    "    df = df.iloc[14:]\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.iloc[1:]\n",
    "    tem_sizes.append(len(df))\n",
    "\n",
    "for elem in mul:\n",
    "    df = pd.read_excel(elem)\n",
    "    df = df.iloc[14:]\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.iloc[1:]\n",
    "    mul_sizes.append(len(df))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oce_sizes, tem_sizes, mul_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(y=[2]*12, x=oce_sizes, c='green', alpha=0.5, s=150, edgecolor='black')\n",
    "ax1.scatter(y=[3]*7, x=mul_sizes, c='red', alpha=0.5, s=150, edgecolor='black')\n",
    "ax1.scatter(y=[1]*4, x=tem_sizes, c='blue', alpha=0.5, s=150, edgecolor='black')\n",
    "#ax1.annotate('Species', xy=(0, 1), xytext=(-50,20), ha='left', va='top', xycoords='axes fraction', textcoords='offset points', fontsize=13)\n",
    "ax1.annotate('Size (in rows)', xy=(0.98, 0), xytext=(1.06,0), ha='left', va='top', xycoords='axes fraction', fontsize=14)\n",
    "plt.yticks([0.5,1,2,3,3.5], labels = ['','T. Temporalis' ,'L. Ocellatus', 'N. Multifasciatus',''])\n",
    "plt.title('Amount and sizes of single datasets', size=14, pad=20)\n",
    "plt.ylabel('Species', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(oce_sizes), sum(oce_sizes)/len(oce_sizes), max(oce_sizes)\n",
    "min(tem_sizes), sum(tem_sizes)/len(tem_sizes), max(tem_sizes)\n",
    "#min(mul_sizes), sum(mul_sizes)/len(mul_sizes), max(mul_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def hamming(G1,G2):\n",
    "    \n",
    "   \n",
    "        \n",
    "    not_in_g1 = np.setdiff1d(G2.nodes(),G1.nodes())\n",
    "    G1.add_nodes_from(not_in_g1)\n",
    "    not_in_g2 = np.setdiff1d(G1.nodes(),G2.nodes())\n",
    "    G2.add_nodes_from(not_in_g2)\n",
    "\n",
    "    N = G1.number_of_nodes()\n",
    "    \n",
    "    adj1 = nx.to_numpy_array(G1)\n",
    "    adj2 = nx.to_numpy_array(G2)\n",
    "\n",
    "    # undirected case: consider only upper triangular\n",
    "    mask = np.triu_indices(N, k=1)\n",
    "\n",
    "    # directed case: consider all but the diagonal\n",
    "    if nx.is_directed(G1) or nx.is_directed(G2):\n",
    "        new_mask = np.tril_indices(N, k=-1)\n",
    "        mask = (np.append(mask[0], new_mask[0]), np.append(mask[1], new_mask[1]))\n",
    "\n",
    "    # only if there are self-loops include the diagonal\n",
    "    # this corrects the implicit denominator of Hamming, which\n",
    "    # should be N^2 for networks with self-loops and N(N-1) for\n",
    "    # those without\n",
    "    if next(nx.selfloop_edges(G1), False) or next(nx.selfloop_edges(G2), False):\n",
    "        new_mask = np.diag_indices(N)\n",
    "        mask = (np.append(mask[0], new_mask[0]), np.append(mask[1], new_mask[1]))\n",
    "\n",
    "    dist = scipy.spatial.distance.hamming(\n",
    "        adj1[mask].flatten(), adj2[mask].flatten()\n",
    "    )\n",
    "\n",
    "    print(adj1)\n",
    "    print(adj2)\n",
    "    print(dist)\n",
    "    \n",
    "    return dist\n",
    "\n",
    "hamming(g1,g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "möglichkeiten für weighted hamming:\n",
    "    \n",
    "    - adjazenzmatrix mit transition probabilities\n",
    "    - n verkleinern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = nx.read_gpickle(\"/Users/nicol/Desktop/bachelorthesis/data/normalized/ocellatus-categories/Ocellatus_03_11_19_1-6(1).gpickle\")\n",
    "g2 = nx.read_gpickle(\"/Users/nicol/Desktop/bachelorthesis/data/normalized/ocellatus-categories/Ocellatus_03_11_19_7-8_2(1).gpickle\")\n",
    "\n",
    "def weighted_frobenius(g1,g2):\n",
    "\n",
    "    #print(nx.get_edge_attributes(g1, label))\n",
    "    not_in_g1 = np.setdiff1d(g2.nodes(),g1.nodes())\n",
    "    g1.add_nodes_from(not_in_g1)\n",
    "    not_in_g2 = np.setdiff1d(g1.nodes(),g2.nodes())\n",
    "    g2.add_nodes_from(not_in_g2)\n",
    "    \n",
    "    adj1 = nx.to_numpy_array(g1, weight=\"label\")\n",
    "    adj2 = nx.to_numpy_array(g2, weight=\"label\")\n",
    "    dist = np.linalg.norm((adj1 - adj2))\n",
    "    print(adj1)\n",
    "    print(adj2)\n",
    "    print(dist)\n",
    "    \n",
    "    return dist\n",
    "\n",
    "weighted_frobenius(g1,g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "g1 = nx.read_gpickle(\"/Users/nicol/Desktop/bachelorthesis/data/normalized/ocellatus-categories/Ocellatus_03_11_19_1-6(1).gpickle\")\n",
    "g2 = nx.read_gpickle(\"/Users/nicol/Desktop/bachelorthesis/data/normalized/ocellatus-categories/Ocellatus_03_11_19_7-8_2(1).gpickle\")\n",
    "\n",
    "\n",
    "\n",
    "def edge_subst(g1edge, g2edge):\n",
    "    cost = abs(g1edge['label']-g2edge['label'])\n",
    "    return cost\n",
    "    \n",
    "def edge_cost(edge):\n",
    "    cost = abs(edge['label'])\n",
    "    return cost\n",
    "\n",
    "def node_cost(node):\n",
    "    print(node)\n",
    "\n",
    "\"\"\"\n",
    "Returns GED \n",
    "\"\"\"\n",
    "def ged(g1,g2):\n",
    "    dist = nx.graph_edit_distance(g1, g2, edge_subst_cost=edge_subst, edge_ins_cost=edge_cost, edge_del_cost=edge_cost, timeout=10)\n",
    "    print(\"done\")\n",
    "    return dist\n",
    "\n",
    "ged(g1,g2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
